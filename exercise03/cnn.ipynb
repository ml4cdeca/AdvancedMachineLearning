{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9K6vEQuzywF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8029,
     "status": "ok",
     "timestamp": 1559388301284,
     "user": {
      "displayName": "Christopher Lüken-Winkels",
      "photoUrl": "",
      "userId": "09800433090533532875"
     },
     "user_tz": -120
    },
    "id": "1XnXUKOFz6kK",
    "outputId": "25ac9a50-1698-4c25-ec62-ad2835f81835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7h0ptLwz4qj"
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)\n",
    "\n",
    "\n",
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)\n",
    "\n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "\n",
    "def model(X, w_h, w_h2, w_o,a, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    #h = rectify(X @ w_h)\n",
    "    h = PRelu(X @ w_h,a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    #h2 = rectify(h @ w_h2)\n",
    "    h2 = PRelu(h @ w_h2,a_2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1465,
     "status": "error",
     "timestamp": 1559392713754,
     "user": {
      "displayName": "Christopher Lüken-Winkels",
      "photoUrl": "",
      "userId": "09800433090533532875"
     },
     "user_tz": -120
    },
    "id": "xrfepKelz2B_",
    "outputId": "862275ca-8f41-4c0c-9655-b4b121f02417"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.3084473609924316\n",
      "Loss: 20.16912078857422\n",
      "Loss: 21.996244430541992\n",
      "Loss: 10.791604995727539\n",
      "Loss: 9.43410587310791\n",
      "Loss: 7.064802646636963\n",
      "Loss: 5.596531867980957\n",
      "Loss: 2.4379665851593018\n",
      "Loss: 2.483964681625366\n",
      "Loss: 1.2743006944656372\n",
      "Loss: 1.1285089254379272\n",
      "Loss: 0.9836878776550293\n",
      "Loss: 0.7834575176239014\n",
      "Loss: 0.8933309316635132\n",
      "Loss: 0.9158243536949158\n",
      "Loss: 0.8274177312850952\n",
      "Loss: 0.6991760730743408\n",
      "Loss: 0.7600440979003906\n",
      "Loss: 0.9970940351486206\n",
      "Loss: 1.0137861967086792\n",
      "Loss: 1.0761635303497314\n",
      "Loss: 1.0505982637405396\n",
      "Loss: 0.8884304165840149\n",
      "Loss: 1.234202265739441\n",
      "Loss: 0.9113507270812988\n",
      "Loss: 0.9358523488044739\n",
      "Loss: 0.880368709564209\n",
      "Loss: 0.8387554287910461\n",
      "Loss: 0.9210190773010254\n",
      "Loss: 0.7196834683418274\n",
      "Loss: 0.640656054019928\n",
      "Loss: 0.7970659136772156\n",
      "Loss: 1.0960174798965454\n",
      "Loss: 1.3423253297805786\n",
      "Loss: 1.5828328132629395\n",
      "Loss: 0.810356080532074\n",
      "Loss: 0.6107978820800781\n",
      "Loss: 0.659767746925354\n",
      "Loss: 0.7076742053031921\n",
      "Loss: 0.8368184566497803\n",
      "Loss: 1.2333965301513672\n",
      "Loss: 1.0152496099472046\n",
      "Loss: 1.2288882732391357\n",
      "Loss: 0.9758979678153992\n",
      "Loss: 2.0121734142303467\n",
      "Loss: 1.7274113893508911\n",
      "Loss: 1.1412947177886963\n",
      "Loss: 1.2881829738616943\n",
      "Loss: 0.9004176259040833\n",
      "Loss: 0.716145932674408\n",
      "Loss: 0.699310302734375\n",
      "Loss: 0.7123668789863586\n",
      "Loss: 0.6440211534500122\n",
      "Loss: 0.6119352579116821\n",
      "Loss: 0.7512762546539307\n",
      "Loss: 0.8734391927719116\n",
      "Loss: 0.5984376072883606\n",
      "Loss: 0.8345922827720642\n",
      "Loss: 1.228589415550232\n",
      "Loss: 1.0856512784957886\n",
      "Loss: 1.130647897720337\n",
      "Loss: 0.6873471140861511\n",
      "Loss: 0.6848293542861938\n",
      "Loss: 0.6618047952651978\n",
      "Loss: 1.0986220836639404\n",
      "Loss: 1.095740795135498\n",
      "Loss: 0.5107483863830566\n",
      "Loss: 0.6163141131401062\n",
      "Loss: 0.5014505386352539\n",
      "Loss: 0.6562175154685974\n",
      "Loss: 0.5451526641845703\n",
      "Loss: 0.6283597350120544\n",
      "Loss: 0.362894743680954\n",
      "Loss: 0.52448570728302\n",
      "Loss: 0.659411609172821\n",
      "Loss: 0.8855758905410767\n",
      "Loss: 0.918243944644928\n",
      "Loss: 1.5286009311676025\n",
      "Loss: 0.6774523258209229\n",
      "Loss: 0.5329716205596924\n",
      "Loss: 0.5957273840904236\n",
      "Loss: 0.5184599161148071\n",
      "Loss: 0.6317375898361206\n",
      "Loss: 0.8695833683013916\n",
      "Loss: 0.6668078899383545\n",
      "Loss: 0.7691798210144043\n",
      "Loss: 0.4810773432254791\n",
      "Loss: 0.5093238949775696\n",
      "Loss: 0.5414038300514221\n",
      "Loss: 0.8358097076416016\n",
      "Loss: 0.7891210913658142\n",
      "Loss: 0.6520772576332092\n",
      "Loss: 0.6346918344497681\n",
      "Loss: 0.3991919755935669\n",
      "Loss: 0.42385178804397583\n",
      "Loss: 0.5153290033340454\n",
      "Loss: 0.49863532185554504\n",
      "Loss: 0.5469247698783875\n",
      "Loss: 0.8132224082946777\n",
      "Loss: 0.9249513745307922\n",
      "Loss: 0.9862449765205383\n",
      "Loss: 1.0226210355758667\n",
      "Loss: 1.043601632118225\n",
      "Loss: 0.7381773591041565\n",
      "Loss: 0.44409722089767456\n",
      "Loss: 0.5784731507301331\n",
      "Loss: 0.39083343744277954\n",
      "Loss: 0.38446763157844543\n",
      "Loss: 0.2739756405353546\n",
      "Loss: 0.21334996819496155\n",
      "Loss: 0.28386005759239197\n",
      "Loss: 0.46290943026542664\n",
      "Loss: 0.6124731302261353\n",
      "Loss: 0.7035075426101685\n",
      "Loss: 0.9362517595291138\n",
      "Loss: 1.329930067062378\n",
      "Loss: 0.7862213850021362\n",
      "Loss: 0.43863528966903687\n",
      "Loss: 0.4003271758556366\n",
      "Loss: 0.6071165204048157\n",
      "Loss: 0.5387146472930908\n",
      "Loss: 0.32486966252326965\n",
      "Loss: 0.39972296357154846\n",
      "Loss: 0.5499864816665649\n",
      "Loss: 0.4499707520008087\n",
      "Loss: 0.32664358615875244\n",
      "Loss: 0.47879326343536377\n",
      "Loss: 0.6877168416976929\n",
      "Loss: 0.4300706386566162\n",
      "Loss: 0.34836146235466003\n",
      "Loss: 0.6758485436439514\n",
      "Loss: 0.5557480454444885\n",
      "Loss: 0.6268404126167297\n",
      "Loss: 0.5717952847480774\n",
      "Loss: 0.4625692069530487\n",
      "Loss: 0.6204176545143127\n",
      "Loss: 0.4652858078479767\n",
      "Loss: 0.19395896792411804\n",
      "Loss: 0.3445965647697449\n",
      "Loss: 1.1855511665344238\n",
      "Loss: 1.0966200828552246\n",
      "Loss: 0.8378573060035706\n",
      "Loss: 1.1216647624969482\n",
      "Loss: 0.7221753001213074\n",
      "Loss: 1.1436667442321777\n",
      "Loss: 0.9458692073822021\n",
      "Loss: 0.4334377944469452\n",
      "Loss: 0.2453804463148117\n",
      "Loss: 0.35844871401786804\n",
      "Loss: 0.3821292519569397\n",
      "Loss: 0.3153562843799591\n",
      "Loss: 0.34652191400527954\n",
      "Loss: 0.34035012125968933\n",
      "Loss: 0.39777418971061707\n",
      "Loss: 0.37398800253868103\n",
      "Loss: 0.34833431243896484\n",
      "Loss: 0.531001627445221\n",
      "Loss: 0.5405191779136658\n",
      "Loss: 0.4213687777519226\n",
      "Loss: 0.5936115980148315\n",
      "Loss: 0.57506263256073\n",
      "Loss: 0.5719115138053894\n",
      "Loss: 0.546302318572998\n",
      "Loss: 0.616736888885498\n",
      "Loss: 0.5150119662284851\n",
      "Loss: 0.26687344908714294\n",
      "Loss: 0.21556739509105682\n",
      "Loss: 0.2402001917362213\n",
      "Loss: 0.3694296181201935\n",
      "Loss: 0.3335428535938263\n",
      "Loss: 0.4178248643875122\n",
      "Loss: 0.599862277507782\n",
      "Loss: 1.7607660293579102\n",
      "Loss: 1.203131079673767\n",
      "Loss: 0.6972779631614685\n",
      "Loss: 0.3397645652294159\n",
      "Loss: 0.3326864242553711\n",
      "Loss: 0.367378830909729\n",
      "Loss: 0.3570800721645355\n",
      "Loss: 0.6521409749984741\n",
      "Loss: 0.42809468507766724\n",
      "Loss: 0.2687782645225525\n",
      "Loss: 0.29834020137786865\n",
      "Loss: 0.3038088083267212\n",
      "Loss: 0.344096302986145\n",
      "Loss: 0.5438936948776245\n",
      "Loss: 0.7134397625923157\n",
      "Loss: 0.39964666962623596\n",
      "Loss: 0.19735677540302277\n",
      "Loss: 0.47577670216560364\n",
      "Loss: 0.4131346046924591\n",
      "Loss: 0.29042747616767883\n",
      "Loss: 0.44306278228759766\n",
      "Loss: 0.42438986897468567\n",
      "Loss: 0.45751845836639404\n",
      "Loss: 0.28734925389289856\n",
      "Loss: 0.3777920603752136\n",
      "Loss: 0.3824503421783447\n",
      "Loss: 0.6430736780166626\n",
      "Loss: 0.6397202610969543\n",
      "Loss: 0.30155861377716064\n",
      "Loss: 0.5026754140853882\n",
      "Loss: 0.5423872470855713\n",
      "Loss: 0.49376311898231506\n",
      "Loss: 0.4859021306037903\n",
      "Loss: 0.4879577159881592\n",
      "Loss: 0.6228474378585815\n",
      "Loss: 0.36639735102653503\n",
      "Loss: 1.2182615995407104\n",
      "Loss: 0.9216813445091248\n",
      "Loss: 0.5928381085395813\n",
      "Loss: 0.25112053751945496\n",
      "Loss: 0.448110967874527\n",
      "Loss: 0.6735246181488037\n",
      "Loss: 0.41997191309928894\n",
      "Loss: 0.3741121292114258\n",
      "Loss: 0.44395729899406433\n",
      "Loss: 0.2811989486217499\n",
      "Loss: 0.21844074130058289\n",
      "Loss: 0.35910385847091675\n",
      "Loss: 0.4968700110912323\n",
      "Loss: 0.31487414240837097\n",
      "Loss: 0.3500184714794159\n",
      "Loss: 0.2974088788032532\n",
      "Loss: 0.4779934585094452\n",
      "Loss: 0.32526978850364685\n",
      "Loss: 0.3395719528198242\n",
      "Loss: 0.5460103750228882\n",
      "Loss: 0.6699002981185913\n",
      "Loss: 0.6288425922393799\n",
      "Loss: 0.41867026686668396\n",
      "Loss: 0.5360532999038696\n",
      "Loss: 0.46141311526298523\n",
      "Loss: 0.34750306606292725\n",
      "Loss: 0.1329277753829956\n",
      "Loss: 0.4313201606273651\n",
      "Loss: 0.15812848508358002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6166e63a1529>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mnoise_py_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_o\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise_py_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e88a5a917f0b>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, w_h, w_h2, w_o, a, p_drop_input, p_drop_hidden)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m#h = rectify(X @ w_h)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPRelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_drop_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m#h2 = rectify(h @ w_h2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPRelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw_h2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-14697b77d47c>\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(X, p_drop)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#np.where(np.random.random(X.shape)<p_drop)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mp_drop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "#prelu constant\n",
    "a = init_weights((1,625))\n",
    "a_2 = init_weights((1,625))\n",
    "optimizer = RMSprop([w_h, w_h2, w_o, a])\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    optimizer.zero_grad()\n",
    "    noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o,a, 0.8, 0.7)\n",
    "    cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "    cost.backward()\n",
    "    #print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ia6CIgvfQBOg"
   },
   "source": [
    "## 3 Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvdSA-xK1mdy"
   },
   "outputs": [],
   "source": [
    "def dropout(X,p_drop=1):\n",
    "    if p_drop<0 or p_drop>1:\n",
    "        return X\n",
    "    mask=np.random.binomial(1,p_drop,X.shape)#np.where(np.random.random(X.shape)<p_drop)\n",
    "    X[mask]=0\n",
    "    return X/p_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cl6E9GhdS3yR"
   },
   "source": [
    "the dropout method sets randomly a certain percentage of the weights to zero. This reduces overfitting as it prevents coadaptive learning. But the implementation here makes the code run way slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQYkOSNRTlLj"
   },
   "source": [
    "## 4 Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Beg5AFMZR5Uv"
   },
   "outputs": [],
   "source": [
    "def PRelu (X,a):\n",
    "    z = torch.tensor(X, requires_grad=True)\n",
    "    z = z*a\n",
    "    z[z >= 0] = X[z >= 0]\n",
    "    return z\n",
    "#TODO include a in params"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
