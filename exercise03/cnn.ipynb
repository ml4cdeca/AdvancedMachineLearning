{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9K6vEQuzywF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8029,
     "status": "ok",
     "timestamp": 1559388301284,
     "user": {
      "displayName": "Christopher LÃ¼ken-Winkels",
      "photoUrl": "",
      "userId": "09800433090533532875"
     },
     "user_tz": -120
    },
    "id": "1XnXUKOFz6kK",
    "outputId": "25ac9a50-1698-4c25-ec62-ad2835f81835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set\n",
    "test_dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = False,\n",
    "                     transform = trans)\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=10000,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7h0ptLwz4qj"
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)\n",
    "\n",
    "\n",
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)\n",
    "\n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    #X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1207
    },
    "colab_type": "code",
    "id": "xrfepKelz2B_",
    "outputId": "d408a11a-affc-4c05-939d-81eec2d69349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 2.4383063316345215\n",
      "Loss: 0.45292049646377563\n",
      "Loss: 0.1624702513217926\n",
      "Loss: 0.314606636762619\n",
      "Loss: 0.37931641936302185\n",
      "Loss: 0.2577272355556488\n",
      "Epoch 2\n",
      "Loss: 0.2254757285118103\n",
      "Loss: 0.177986279129982\n",
      "Loss: 0.09745069593191147\n",
      "Loss: 0.1130680963397026\n",
      "Loss: 0.16060322523117065\n",
      "Loss: 0.13469071686267853\n",
      "Epoch 3\n",
      "Loss: 0.08190114051103592\n",
      "Loss: 0.16590620577335358\n",
      "Loss: 0.1779492199420929\n",
      "Loss: 0.03844417631626129\n",
      "Loss: 0.07598592340946198\n",
      "Loss: 0.19877488911151886\n",
      "Epoch 4\n",
      "Loss: 0.07959254086017609\n",
      "Loss: 0.05046374723315239\n",
      "Loss: 0.11108023673295975\n",
      "Loss: 0.09400580078363419\n",
      "Loss: 0.10574810951948166\n",
      "Loss: 0.0358109287917614\n",
      "Epoch 5\n",
      "Loss: 0.1245107650756836\n",
      "Loss: 0.06308893859386444\n",
      "Loss: 0.04351544380187988\n",
      "Loss: 0.1330675184726715\n",
      "Loss: 0.02346799522638321\n",
      "Loss: 0.05445188656449318\n",
      "Epoch 6\n",
      "Loss: 0.06675520539283752\n",
      "Loss: 0.12438568472862244\n",
      "Loss: 0.21986620128154755\n",
      "Loss: 0.07772628217935562\n",
      "Loss: 0.018264278769493103\n",
      "Loss: 0.02790614403784275\n",
      "Epoch 7\n",
      "Loss: 0.16228385269641876\n",
      "Loss: 0.1180594339966774\n",
      "Loss: 0.05563650280237198\n",
      "Loss: 0.06450846791267395\n",
      "Loss: 0.07697255909442902\n",
      "Loss: 0.0648568794131279\n",
      "Epoch 8\n",
      "Loss: 0.163353830575943\n",
      "Loss: 0.10501360148191452\n",
      "Loss: 0.043924618512392044\n",
      "Loss: 0.01800457388162613\n",
      "Loss: 0.025225548073649406\n",
      "Loss: 0.0027614973951131105\n",
      "Epoch 9\n",
      "Loss: 0.0944112241268158\n",
      "Loss: 0.12031122297048569\n",
      "Loss: 0.053418733179569244\n",
      "Loss: 0.023077674210071564\n",
      "Loss: 0.02035583183169365\n",
      "Loss: 0.020819874480366707\n",
      "Epoch 10\n",
      "Loss: 0.016732744872570038\n",
      "Loss: 0.12447673827409744\n",
      "Loss: 0.07921209931373596\n",
      "Loss: 0.10111520439386368\n",
      "Loss: 0.045376405119895935\n",
      "Loss: 0.009396100416779518\n"
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "for i in range(10):\n",
    "    print('Epoch %i'%(i+1))\n",
    "    for (j, (X, y)) in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "        cost.backward()\n",
    "        if j%100==0:\n",
    "            print(\"Loss: {}\".format(cost))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate test accuracy\n",
    "def test_acc(weights,model):\n",
    "    for (_, (X, y)) in enumerate(test_dataloader, 0):\n",
    "        pred=torch.max(model(X.reshape(test_dataloader.batch_size, -1), *weights,),1)[1]\n",
    "        err=np.count_nonzero(pred.numpy()==y.numpy())/test_dataloader.batch_size\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gXtiz0c4LaCA",
    "outputId": "ef99a1e3-b07f-48cd-fc67-e08b4810d29d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc([ w_h, w_h2, w_o, 0.8, 0.7],model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial test error: 97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ia6CIgvfQBOg"
   },
   "source": [
    "## With Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvdSA-xK1mdy"
   },
   "outputs": [],
   "source": [
    "def dropout(X,p_drop=1):\n",
    "    if p_drop<0 or p_drop>1:\n",
    "        return X\n",
    "    mask=np.random.binomial(1,p_drop,X.shape)\n",
    "    X = X * torch.tensor(mask).type(torch.FloatTensor)\n",
    "    return X/p_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(X @ w_h)\n",
    "    #h = PRelu(X @ w_h,a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    #h2 = PRelu(h @ w_h2,a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cl6E9GhdS3yR"
   },
   "source": [
    "the dropout method sets randomly a certain percentage of the weights to zero. This reduces overfitting as it prevents that specific weights counter act each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1207
    },
    "colab_type": "code",
    "id": "hir-qhZHWsue",
    "outputId": "f4911ac6-1014-49bb-eae3-f11361be34e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 2.870680332183838\n",
      "Loss: 0.9895469546318054\n",
      "Loss: 0.44895243644714355\n",
      "Loss: 0.35955145955085754\n",
      "Loss: 0.42591896653175354\n",
      "Loss: 0.3463113009929657\n",
      "Epoch 2\n",
      "Loss: 0.3647099733352661\n",
      "Loss: 0.5683982968330383\n",
      "Loss: 0.23462322354316711\n",
      "Loss: 0.36871135234832764\n",
      "Loss: 0.29702872037887573\n",
      "Loss: 0.26043036580085754\n",
      "Epoch 3\n",
      "Loss: 0.21323558688163757\n",
      "Loss: 0.12817656993865967\n",
      "Loss: 0.0769650861620903\n",
      "Loss: 0.1589348167181015\n",
      "Loss: 0.10127487033605576\n",
      "Loss: 0.17777320742607117\n",
      "Epoch 4\n",
      "Loss: 0.11273083090782166\n",
      "Loss: 0.14847451448440552\n",
      "Loss: 0.2707211375236511\n",
      "Loss: 0.4120597839355469\n",
      "Loss: 0.1962129920721054\n",
      "Loss: 0.23093529045581818\n",
      "Epoch 5\n",
      "Loss: 0.08885368704795837\n",
      "Loss: 0.31845036149024963\n",
      "Loss: 0.1479317545890808\n",
      "Loss: 0.11292293667793274\n",
      "Loss: 0.29015690088272095\n",
      "Loss: 0.28360989689826965\n",
      "Epoch 6\n",
      "Loss: 0.12882612645626068\n",
      "Loss: 0.2815173268318176\n",
      "Loss: 0.06088335067033768\n",
      "Loss: 0.2516140043735504\n",
      "Loss: 0.22176986932754517\n",
      "Loss: 0.10832227766513824\n",
      "Epoch 7\n",
      "Loss: 0.2505129873752594\n",
      "Loss: 0.28012123703956604\n",
      "Loss: 0.052844591438770294\n",
      "Loss: 0.34634506702423096\n",
      "Loss: 0.15639609098434448\n",
      "Loss: 0.23346498608589172\n",
      "Epoch 8\n",
      "Loss: 0.32799819111824036\n",
      "Loss: 0.07590657472610474\n",
      "Loss: 0.22888325154781342\n",
      "Loss: 0.07507529854774475\n",
      "Loss: 0.2783103883266449\n",
      "Loss: 0.06843138486146927\n",
      "Epoch 9\n",
      "Loss: 0.17819654941558838\n",
      "Loss: 0.09240005165338516\n",
      "Loss: 0.07075738906860352\n",
      "Loss: 0.3454631567001343\n",
      "Loss: 0.2967265844345093\n",
      "Loss: 0.1594192534685135\n",
      "Epoch 10\n",
      "Loss: 0.08341965824365616\n",
      "Loss: 0.24978803098201752\n",
      "Loss: 0.15224622189998627\n",
      "Loss: 0.15845933556556702\n",
      "Loss: 0.22823753952980042\n",
      "Loss: 0.1679150015115738\n"
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "optimizer = RMSprop([w_h, w_h2, w_o])\n",
    "\n",
    "for i in range(10):\n",
    "    print('Epoch %i'%(i+1))\n",
    "    for (j, (X, y)) in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o, 0.8, 0.7)\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "        cost.backward()\n",
    "        if j%100==0:\n",
    "            print(\"Loss: {}\".format(cost))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dTwPg5sKXb0w",
    "outputId": "b89992e7-690a-424c-cb60-05d06c6e4000"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc([w_h, w_h2, w_o, 0.8, 0.7],model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQYkOSNRTlLj"
   },
   "source": [
    "## With Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRelu (X,a):\n",
    "    z = torch.tensor(X, requires_grad=True)\n",
    "    z = z*a[0]\n",
    "    z[z >= 0] = X[z >= 0]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggixM1HhXZBJ"
   },
   "outputs": [],
   "source": [
    "def model(X, w_h, w_h2, w_o, a,p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = PRelu(X @ w_h,a)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = PRelu(h @ w_h2,a)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "g8lpWY-3XatD",
    "outputId": "3f407a4e-3192-4171-9eee-63166c726b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 2.8057448863983154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9584658741950989\n",
      "Loss: 0.5390554666519165\n",
      "Loss: 0.43708187341690063\n",
      "Loss: 0.552754282951355\n",
      "Loss: 0.6651925444602966\n",
      "Epoch 2\n",
      "Loss: 0.42307403683662415\n",
      "Loss: 0.5108500123023987\n",
      "Loss: 0.6477224230766296\n",
      "Loss: 0.4062381088733673\n",
      "Loss: 0.5530341863632202\n",
      "Loss: 0.34933072328567505\n",
      "Epoch 3\n",
      "Loss: 0.2801732122898102\n",
      "Loss: 0.5084130764007568\n",
      "Loss: 0.3599422574043274\n",
      "Loss: 0.3402005732059479\n",
      "Loss: 0.2535078823566437\n",
      "Loss: 0.4418446719646454\n",
      "Epoch 4\n",
      "Loss: 0.40707945823669434\n",
      "Loss: 0.3395109176635742\n",
      "Loss: 0.5701867938041687\n",
      "Loss: 0.22224725782871246\n",
      "Loss: 0.499553918838501\n",
      "Loss: 0.2131744921207428\n",
      "Epoch 5\n",
      "Loss: 0.2767999768257141\n",
      "Loss: 0.26099830865859985\n",
      "Loss: 0.3587254285812378\n",
      "Loss: 0.40119126439094543\n",
      "Loss: 0.21230147778987885\n",
      "Loss: 0.42506009340286255\n",
      "Epoch 6\n",
      "Loss: 0.19191144406795502\n",
      "Loss: 0.515203058719635\n",
      "Loss: 0.3134538531303406\n",
      "Loss: 0.5087266564369202\n",
      "Loss: 0.33230167627334595\n",
      "Loss: 0.5998152494430542\n",
      "Epoch 7\n",
      "Loss: 0.13249428570270538\n",
      "Loss: 0.17606639862060547\n",
      "Loss: 0.44741538166999817\n",
      "Loss: 0.2082759439945221\n",
      "Loss: 0.19587399065494537\n",
      "Loss: 0.23532383143901825\n",
      "Epoch 8\n",
      "Loss: 0.20838521420955658\n",
      "Loss: 0.2687400281429291\n",
      "Loss: 0.3140673339366913\n",
      "Loss: 0.24767127633094788\n",
      "Loss: 0.4601421356201172\n",
      "Loss: 0.2727713882923126\n",
      "Epoch 9\n",
      "Loss: 0.24285970628261566\n",
      "Loss: 0.20170511305332184\n",
      "Loss: 0.3201330304145813\n",
      "Loss: 0.33158981800079346\n",
      "Loss: 0.27310577034950256\n",
      "Loss: 0.2271476686000824\n",
      "Epoch 10\n",
      "Loss: 0.2988394498825073\n",
      "Loss: 0.22932255268096924\n",
      "Loss: 0.38150936365127563\n",
      "Loss: 0.24104465544223785\n",
      "Loss: 0.3917209506034851\n",
      "Loss: 0.18944081664085388\n"
     ]
    }
   ],
   "source": [
    "w_h = init_weights((784, 625))\n",
    "w_h2 = init_weights((625, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "#prelu constant\n",
    "a_2 = init_weights((1,625))\n",
    "optimizer = RMSprop([*w_conv1, w_h2, w_o, a_2])\n",
    "\n",
    "for i in range(10):\n",
    "    print('Epoch %i'%(i+1))\n",
    "    for (j, (X, y)) in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        noise_py_x = model(X.reshape(mb_size, 784), w_h, w_h2, w_o,a_2, 0.8, 0.7)\n",
    "        cost = torch.nn.functional.cross_entropy(noise_py_x, y)\n",
    "        cost.backward()\n",
    "        if j%100==0:\n",
    "            print(\"Loss: {}\".format(cost))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "zBhpDreLXgM0",
    "outputId": "b9201898-090f-4313-cc21-71fbd532f668"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error([w_h, w_h2, w_o,a_2, 0.8, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, w_conv1, w_h2, w_o, a_2, p_drop_input, p_drop_hidden,number_of_output_pixel=128):\n",
    "    X = X.reshape(-1, 1, 28, 28)\n",
    "    X = dropout(X, p_drop_input)\n",
    "    for i in range(len(w_conv1)):\n",
    "        convolutional_layer = rectify(conv2d(X, w_conv1[i] ))\n",
    "        subsample_layer = max_pool2d(convolutional_layer, (2, 2)) # reduces window 2x2 to 1 pixel\n",
    "        X = dropout(subsample_layer, p_drop_input )\n",
    "    X = X.reshape((-1,number_of_output_pixel))\n",
    "    h2 = PRelu(X @ w_h2,a_2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "xrfepKelz2B_",
    "outputId": "27a56101-c1fd-4ff7-df25-93114445414a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 17.151647567749023\n",
      "100 Loss: 1.1014100313186646\n",
      "200 Loss: 0.7716077566146851\n",
      "300 Loss: 0.48061081767082214\n",
      "400 Loss: 0.401603102684021\n",
      "500 Loss: 0.3198795020580292\n",
      "Epoch 1\n",
      "0 Loss: 0.4496510326862335\n",
      "100 Loss: 0.4620377719402313\n",
      "200 Loss: 0.34185177087783813\n",
      "300 Loss: 0.29438361525535583\n",
      "400 Loss: 0.2602473199367523\n",
      "500 Loss: 0.17138339579105377\n",
      "Epoch 2\n",
      "0 Loss: 0.1372532993555069\n",
      "100 Loss: 0.16332699358463287\n",
      "200 Loss: 0.14734528958797455\n",
      "300 Loss: 0.42788660526275635\n",
      "400 Loss: 0.1815611720085144\n",
      "500 Loss: 0.16142334043979645\n",
      "Epoch 3\n",
      "0 Loss: 0.20599357783794403\n",
      "100 Loss: 0.19268590211868286\n",
      "200 Loss: 0.20944072306156158\n",
      "300 Loss: 0.2470216155052185\n",
      "400 Loss: 0.1767912060022354\n",
      "500 Loss: 0.07982588559389114\n",
      "Epoch 4\n",
      "0 Loss: 0.3144785761833191\n",
      "100 Loss: 0.0958995521068573\n",
      "200 Loss: 0.49994003772735596\n",
      "300 Loss: 0.06696636229753494\n",
      "400 Loss: 0.2954217791557312\n",
      "500 Loss: 0.12420207262039185\n",
      "Epoch 5\n",
      "0 Loss: 0.11941450089216232\n",
      "100 Loss: 0.2950465679168701\n",
      "200 Loss: 0.2356269806623459\n",
      "300 Loss: 0.1162613108754158\n",
      "400 Loss: 0.07860050350427628\n",
      "500 Loss: 0.09924814105033875\n",
      "Epoch 6\n",
      "0 Loss: 0.13338817656040192\n",
      "100 Loss: 0.10457376390695572\n",
      "200 Loss: 0.08822624385356903\n",
      "300 Loss: 0.07214728742837906\n",
      "400 Loss: 0.2984665632247925\n",
      "500 Loss: 0.22554324567317963\n",
      "Epoch 7\n",
      "0 Loss: 0.14312279224395752\n",
      "100 Loss: 0.0696079283952713\n",
      "200 Loss: 0.1591915637254715\n",
      "300 Loss: 0.18785904347896576\n",
      "400 Loss: 0.10334476828575134\n",
      "500 Loss: 0.2978014349937439\n",
      "Epoch 8\n",
      "0 Loss: 0.0715358555316925\n",
      "100 Loss: 0.09592942893505096\n",
      "200 Loss: 0.12955524027347565\n",
      "300 Loss: 0.10487077385187149\n",
      "400 Loss: 0.12165097892284393\n",
      "500 Loss: 0.2584969401359558\n",
      "Epoch 9\n",
      "0 Loss: 0.033948373049497604\n",
      "100 Loss: 0.1246253177523613\n",
      "200 Loss: 0.17588689923286438\n",
      "300 Loss: 0.11587578803300858\n",
      "400 Loss: 0.1435782015323639\n",
      "500 Loss: 0.12869060039520264\n"
     ]
    }
   ],
   "source": [
    "w_1 = init_weights((32, 1, 5, 5))\n",
    "w_2 = init_weights((64, 32, 5, 5))\n",
    "w_3 = init_weights((128, 64, 2, 2))\n",
    "w_conv1 = [w_1, w_2, w_3]\n",
    "\n",
    "w_h2 = init_weights((128, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "#prelu constant\n",
    "a_2 = init_weights((1,625))\n",
    "optimizer = RMSprop([*w_conv1, w_h2, w_o, a_2])\n",
    "\n",
    "for i in range(10):\n",
    "    print('Epoch %i'%i)\n",
    "    for (j, (X, y)) in enumerate(dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        noise = torch.tensor([])\n",
    "        noise=model(X.reshape(mb_size, 784), w_conv1, w_h2, w_o, a_2, 0.8, 0.7)\n",
    "        cost = torch.nn.functional.cross_entropy(noise, y)\n",
    "        cost.backward()\n",
    "        if j%100==0:\n",
    "            print(j,\"Loss: {}\".format(cost))\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "6NKr88sa_cMi",
    "outputId": "1566e396-0e04-4339-d6e1-f0c6d1fdd5e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9686"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc([w_conv1, w_h2, w_o, a_2, 0.8, 0.7],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xG4ICTSxLDa"
   },
   "outputs": [],
   "source": [
    "def conv_plot(X, w_conv1, w_h2, w_o, a_2, p_drop_input, p_drop_hidden):\n",
    "    X = X.reshape(-1, 1, 28, 28)\n",
    "    conv=None\n",
    "    X = dropout(X, p_drop_input)\n",
    "    for i in range(len(w_conv1)):\n",
    "        convolutional_layer = rectify(conv2d(X, w_conv1[i] ))\n",
    "        X = max_pool2d(convolutional_layer, (2, 2)) # reduces window 2x2 to 1 pixel\n",
    "        if i==0:\n",
    "            conv=X\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "be998CPW3Ry5"
   },
   "outputs": [],
   "source": [
    "#one image from the test set:\n",
    "image=test_dataset[0][0]\n",
    "conv=conv_plot(image, w_conv1, w_h2, w_o, a_2, 0.8, 0.7).detach().numpy().reshape(32,12,12)[:3,:,:]\n",
    "image=image.numpy().reshape(28,28)\n",
    "filters=w_1.detach().numpy().reshape(32,5,5)[:3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "mCfIAkMs3169",
    "outputId": "0f2dd87f-9b64-4f54-d9d7-a75047646726"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAE8CAYAAAAPPI/5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 7 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=[image,*conv,*filters,]\n",
    "title=['test image','conv. 1','conv. 2','conv. 3','filter 1','filter 2','filter 3']\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in range(7):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(data[i],cmap='gray')\n",
    "    plt.title(title[i])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove one convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "sLnreQPR5fri",
    "outputId": "ac5887ba-7b4e-4beb-a6c7-58f037d6d131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 8.198566436767578\n",
      "100 Loss: 0.25211766362190247\n",
      "200 Loss: 0.15495595335960388\n",
      "300 Loss: 0.05368582159280777\n",
      "400 Loss: 0.10144832730293274\n",
      "500 Loss: 0.05655602365732193\n",
      "Epoch 1\n",
      "0 Loss: 0.045855067670345306\n",
      "100 Loss: 0.012210994027554989\n",
      "200 Loss: 0.07361559569835663\n",
      "300 Loss: 0.05953839421272278\n",
      "400 Loss: 0.21496132016181946\n",
      "500 Loss: 0.2773759067058563\n",
      "Epoch 2\n",
      "0 Loss: 0.15119895339012146\n",
      "100 Loss: 0.034387730062007904\n",
      "200 Loss: 0.13777832686901093\n",
      "300 Loss: 0.04217636212706566\n",
      "400 Loss: 0.08751045167446136\n",
      "500 Loss: 0.17050892114639282\n",
      "Epoch 3\n",
      "0 Loss: 0.03309304639697075\n",
      "100 Loss: 0.030630258843302727\n",
      "200 Loss: 0.0879826620221138\n",
      "300 Loss: 0.05866178870201111\n",
      "400 Loss: 0.061996642500162125\n",
      "500 Loss: 0.1360815167427063\n",
      "Epoch 4\n",
      "0 Loss: 0.17656266689300537\n",
      "100 Loss: 0.051665037870407104\n",
      "200 Loss: 0.19343572854995728\n",
      "300 Loss: 0.1799963414669037\n",
      "400 Loss: 0.07805664837360382\n",
      "500 Loss: 0.002313323086127639\n",
      "Epoch 5\n",
      "0 Loss: 0.12455422431230545\n",
      "100 Loss: 0.03645532205700874\n",
      "200 Loss: 0.08393168449401855\n",
      "300 Loss: 0.22306706011295319\n",
      "400 Loss: 0.06904686987400055\n",
      "500 Loss: 0.10877031087875366\n",
      "Epoch 6\n",
      "0 Loss: 0.04868330433964729\n",
      "100 Loss: 0.054600391536951065\n",
      "200 Loss: 0.03474034368991852\n",
      "300 Loss: 0.07769210636615753\n",
      "400 Loss: 0.05838775634765625\n",
      "500 Loss: 0.011628635227680206\n",
      "Epoch 7\n",
      "0 Loss: 0.11937455832958221\n",
      "100 Loss: 0.11028384417295456\n",
      "200 Loss: 0.011272664181888103\n",
      "300 Loss: 0.05904049053788185\n",
      "400 Loss: 0.08849038183689117\n",
      "500 Loss: 0.040305547416210175\n",
      "Epoch 8\n",
      "0 Loss: 0.1174497902393341\n",
      "100 Loss: 0.026292255148291588\n",
      "200 Loss: 0.14321111142635345\n",
      "300 Loss: 0.10535066574811935\n",
      "400 Loss: 0.003387879114598036\n",
      "500 Loss: 0.0706702172756195\n",
      "Epoch 9\n",
      "0 Loss: 0.023569492623209953\n",
      "100 Loss: 0.035683825612068176\n",
      "200 Loss: 0.015552406199276447\n",
      "300 Loss: 0.026423029601573944\n",
      "400 Loss: 0.14622649550437927\n",
      "500 Loss: 0.026266440749168396\n"
     ]
    }
   ],
   "source": [
    "#model with fewer convolutional layers\n",
    "w_1_new = init_weights((32, 1, 5, 5))\n",
    "w_3_new = init_weights((128, 32, 3,3))\n",
    "w_conv1_new = [w_1_new, w_3_new]\n",
    "\n",
    "w_h2_new = init_weights((128*25, 625))\n",
    "w_o_new = init_weights((625, 10))\n",
    "#prelu constant\n",
    "a_2_new = init_weights((1,625))\n",
    "optimizer_new = RMSprop([*w_conv1_new, w_h2_new, w_o_new, a_2_new])\n",
    "\n",
    "for i in range(10):\n",
    "    print('Epoch %i'%i)\n",
    "    for (j, (X, y)) in enumerate(dataloader, 0):\n",
    "        optimizer_new.zero_grad()\n",
    "        noise = torch.tensor([])\n",
    "        noise=model(X.reshape(mb_size, -1), w_conv1_new, w_h2_new, w_o_new, a_2_new, 0.8, 0.7,number_of_output_pixel=3200)\n",
    "        cost = torch.nn.functional.cross_entropy(noise, y)\n",
    "        cost.backward()\n",
    "        if j%100==0:\n",
    "            print(j,\"Loss: {}\".format(cost))\n",
    "        optimizer_new.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "SKoQoNYqJqwa",
    "outputId": "246a859e-1666-4ef6-cdd4-93f04b683393"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc([ w_conv1_new, w_h2_new, w_o_new, a_2_new, 0.8, 0.7,3200],model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                    | Test accuracy |\n",
    "|--------------------------|------------|\n",
    "| Standard                 | 97.0%      |\n",
    "| Dropout                  |            |\n",
    "| Prelu                    |            |\n",
    "| Convolutional (3 layers) | 96.9%      |\n",
    "| Convolutional (2 layers) | 98.2%      |\n",
    "\n",
    "All of the methods had a high test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
