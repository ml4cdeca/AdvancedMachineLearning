{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9K6vEQuzywF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8029,
     "status": "ok",
     "timestamp": 1559388301284,
     "user": {
      "displayName": "Christopher Lüken-Winkels",
      "photoUrl": "",
      "userId": "09800433090533532875"
     },
     "user_tz": -120
    },
    "id": "1XnXUKOFz6kK",
    "outputId": "25ac9a50-1698-4c25-ec62-ad2835f81835"
   },
   "outputs": [],
   "source": [
    "mb_size = 100 # mini-batch size of 100\n",
    "\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "dataset = dset.MNIST(\"./\", download = True,\n",
    "                     train = True,\n",
    "                     transform = trans)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=mb_size,\n",
    "                                         shuffle=True, num_workers=1,\n",
    "                                         pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7h0ptLwz4qj"
   },
   "outputs": [],
   "source": [
    "def dropout(X,p_drop=1):\n",
    "    if p_drop<0 or p_drop>1:\n",
    "        return X\n",
    "    mask=np.random.binomial(1,p_drop,X.shape)#np.where(np.random.random(X.shape)<p_drop)\n",
    "    X = X * torch.tensor(mask).type(torch.FloatTensor)\n",
    "    return X/p_drop\n",
    "\n",
    "def PRelu (X,a):\n",
    "    z = torch.tensor(X, requires_grad=True)\n",
    "    z = z*a[0]\n",
    "    z[z >= 0] = X[z >= 0]\n",
    "    return z\n",
    "\n",
    "def init_weights(shape):\n",
    "    # xavier initialization (a good initialization is important!)\n",
    "    # http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "    fan_in = shape[0]\n",
    "    fan_out = shape[1]\n",
    "    variance = 2.0/(fan_in + fan_out)\n",
    "    w = torch.randn(size=shape)*np.sqrt(variance)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def rectify(X):\n",
    "    return torch.max(torch.zeros_like(X), X)\n",
    "\n",
    "\n",
    "# you can also use torch.nn.functional.softmax on future sheets\n",
    "def softmax(X):\n",
    "    c = torch.max(X, dim=1)[0].reshape(mb_size, 1)\n",
    "    # this avoids a blow up of the exponentials\n",
    "    # but calculates the same formula\n",
    "    stabelized = X-c\n",
    "    exp = torch.exp(stabelized)\n",
    "    return exp/torch.sum(exp, dim=1).reshape(mb_size, 1)\n",
    "\n",
    "\n",
    "# this is an example as a reduced version of the pytorch internal RMSprop optimizer\n",
    "class RMSprop(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "\n",
    "def model(X, w_conv1, w_h2, w_o, a_2, p_drop_input, p_drop_hidden):\n",
    "    X = X.reshape(-1, 1, 28, 28)\n",
    "    X = dropout(X, p_drop_input)\n",
    "    #h = rectify(X @ w_h)\n",
    "    for i in range(len(w_conv1)):\n",
    "        convolutional_layer = rectify(conv2d(X, w_conv1[i] ))\n",
    "        subsample_layer = max_pool2d(convolutional_layer, (2, 2)) # reduces window 2x2 to 1 pixel\n",
    "        X = dropout(subsample_layer, p_drop_input )\n",
    "    \n",
    "    X = X.reshape((128))\n",
    "    #h2 = rectify(h @ w_h2)\n",
    "    h2 = PRelu(X @ w_h2,a_2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1465,
     "status": "error",
     "timestamp": 1559392713754,
     "user": {
      "displayName": "Christopher Lüken-Winkels",
      "photoUrl": "",
      "userId": "09800433090533532875"
     },
     "user_tz": -120
    },
    "id": "xrfepKelz2B_",
    "outputId": "862275ca-8f41-4c0c-9655-b4b121f02417"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9.076483726501465\n",
      "Loss: 21.814857482910156\n",
      "Loss: 13.486246109008789\n",
      "Loss: 7.855291366577148\n",
      "Loss: 5.60434627532959\n",
      "Loss: 5.221518516540527\n",
      "Loss: 4.083083152770996\n",
      "Loss: 3.5060172080993652\n",
      "Loss: 3.0871341228485107\n",
      "Loss: 2.9395751953125\n",
      "Loss: 3.1969616413116455\n",
      "Loss: 2.980170249938965\n",
      "Loss: 2.7726821899414062\n",
      "Loss: 2.466749668121338\n",
      "Loss: 2.4642858505249023\n",
      "Loss: 2.626936435699463\n",
      "Loss: 2.359334707260132\n",
      "Loss: 2.495028495788574\n",
      "Loss: 2.4967434406280518\n",
      "Loss: 2.5585427284240723\n",
      "Loss: 2.4977221488952637\n",
      "Loss: 2.146055221557617\n",
      "Loss: 2.2822954654693604\n",
      "Loss: 2.1539268493652344\n",
      "Loss: 2.3975000381469727\n",
      "Loss: 2.170358657836914\n",
      "Loss: 1.8855329751968384\n",
      "Loss: 2.340444803237915\n",
      "Loss: 2.322434902191162\n",
      "Loss: 2.27750301361084\n",
      "Loss: 2.1462085247039795\n",
      "Loss: 2.4409046173095703\n",
      "Loss: 2.173182487487793\n",
      "Loss: 2.0850517749786377\n",
      "Loss: 2.035439968109131\n",
      "Loss: 1.958567500114441\n",
      "Loss: 2.4515392780303955\n",
      "Loss: 2.1142237186431885\n",
      "Loss: 1.8617095947265625\n",
      "Loss: 1.9060800075531006\n",
      "Loss: 1.8656036853790283\n",
      "Loss: 1.978005051612854\n",
      "Loss: 1.7487760782241821\n",
      "Loss: 1.7944283485412598\n",
      "Loss: 1.9793490171432495\n",
      "Loss: 1.5541397333145142\n",
      "Loss: 2.0057148933410645\n",
      "Loss: 2.0346322059631348\n",
      "Loss: 1.947320580482483\n",
      "Loss: 1.8002690076828003\n",
      "Loss: 1.7317020893096924\n",
      "Loss: 1.630881905555725\n",
      "Loss: 1.6478638648986816\n",
      "Loss: 1.668268084526062\n",
      "Loss: 1.6403095722198486\n",
      "Loss: 1.5875698328018188\n",
      "Loss: 1.7468507289886475\n",
      "Loss: 1.6319527626037598\n",
      "Loss: 1.6203073263168335\n",
      "Loss: 1.5798771381378174\n",
      "Loss: 1.8300491571426392\n",
      "Loss: 1.5271142721176147\n",
      "Loss: 1.837788701057434\n",
      "Loss: 1.8627526760101318\n",
      "Loss: 1.5324294567108154\n",
      "Loss: 1.6358880996704102\n",
      "Loss: 1.4895204305648804\n",
      "Loss: 1.3542070388793945\n",
      "Loss: 1.5285649299621582\n",
      "Loss: 1.2841558456420898\n",
      "Loss: 1.650868535041809\n",
      "Loss: 1.5859692096710205\n",
      "Loss: 1.6096075773239136\n",
      "Loss: 1.4165343046188354\n",
      "Loss: 1.5297198295593262\n",
      "Loss: 1.2316734790802002\n",
      "Loss: 1.185935616493225\n",
      "Loss: 1.3790026903152466\n",
      "Loss: 1.1539502143859863\n",
      "Loss: 1.4848359823226929\n",
      "Loss: 1.3605769872665405\n",
      "Loss: 1.3599631786346436\n",
      "Loss: 1.5336872339248657\n",
      "Loss: 1.4457786083221436\n",
      "Loss: 1.358237862586975\n",
      "Loss: 1.4315478801727295\n",
      "Loss: 1.2818034887313843\n",
      "Loss: 1.6002126932144165\n",
      "Loss: 1.3503798246383667\n",
      "Loss: 1.2670609951019287\n",
      "Loss: 1.1067557334899902\n",
      "Loss: 1.236006498336792\n",
      "Loss: 1.1672812700271606\n",
      "Loss: 1.1800358295440674\n",
      "Loss: 1.3235496282577515\n",
      "Loss: 1.5070842504501343\n",
      "Loss: 1.5311973094940186\n",
      "Loss: 1.2661781311035156\n",
      "Loss: 1.2244185209274292\n",
      "Loss: 1.2946008443832397\n",
      "Loss: 1.2181057929992676\n",
      "Loss: 1.3605520725250244\n",
      "Loss: 1.6880722045898438\n",
      "Loss: 1.5391064882278442\n",
      "Loss: 1.3457876443862915\n",
      "Loss: 1.1114140748977661\n",
      "Loss: 0.701357901096344\n",
      "Loss: 1.0108940601348877\n",
      "Loss: 1.3606491088867188\n",
      "Loss: 1.1769825220108032\n",
      "Loss: 1.259999394416809\n",
      "Loss: 1.3910679817199707\n",
      "Loss: 1.2452281713485718\n",
      "Loss: 1.1672956943511963\n",
      "Loss: 1.0327086448669434\n",
      "Loss: 1.0406254529953003\n",
      "Loss: 0.9613493084907532\n",
      "Loss: 0.8293102979660034\n",
      "Loss: 1.0906577110290527\n",
      "Loss: 1.047249674797058\n",
      "Loss: 0.8949329257011414\n",
      "Loss: 1.0566842555999756\n",
      "Loss: 1.2804992198944092\n",
      "Loss: 0.9444735646247864\n",
      "Loss: 1.1967099905014038\n",
      "Loss: 0.9904957413673401\n",
      "Loss: 1.322951078414917\n",
      "Loss: 1.495069980621338\n",
      "Loss: 0.9574558138847351\n",
      "Loss: 1.0580503940582275\n",
      "Loss: 0.7969213724136353\n",
      "Loss: 0.9543896317481995\n",
      "Loss: 0.9617429375648499\n",
      "Loss: 1.0042760372161865\n",
      "Loss: 0.9359729886054993\n",
      "Loss: 0.9384120106697083\n",
      "Loss: 1.1807116270065308\n",
      "Loss: 0.9681551456451416\n",
      "Loss: 0.7527937293052673\n",
      "Loss: 0.899501621723175\n",
      "Loss: 0.8418698310852051\n",
      "Loss: 1.130204439163208\n",
      "Loss: 1.1246086359024048\n",
      "Loss: 0.8853437304496765\n",
      "Loss: 1.077539324760437\n",
      "Loss: 0.9691293835639954\n",
      "Loss: 1.0452407598495483\n",
      "Loss: 0.9540130496025085\n",
      "Loss: 0.7614629864692688\n",
      "Loss: 0.8682271838188171\n",
      "Loss: 0.7017490863800049\n",
      "Loss: 0.9725215435028076\n",
      "Loss: 1.1370863914489746\n",
      "Loss: 0.8285275101661682\n",
      "Loss: 0.9267395734786987\n",
      "Loss: 0.8129278421401978\n",
      "Loss: 0.8332280516624451\n",
      "Loss: 0.8260418772697449\n",
      "Loss: 0.968595564365387\n",
      "Loss: 0.8293890953063965\n",
      "Loss: 0.8565657734870911\n",
      "Loss: 1.1668407917022705\n",
      "Loss: 1.1048617362976074\n",
      "Loss: 0.7579862475395203\n",
      "Loss: 0.9733835458755493\n",
      "Loss: 0.8680078983306885\n",
      "Loss: 0.8466399908065796\n",
      "Loss: 0.8912907242774963\n",
      "Loss: 0.8166000247001648\n",
      "Loss: 0.9772646427154541\n",
      "Loss: 0.7080501317977905\n",
      "Loss: 0.5227176547050476\n",
      "Loss: 1.1035436391830444\n",
      "Loss: 0.9934738278388977\n",
      "Loss: 0.9986270070075989\n",
      "Loss: 0.7956691980361938\n",
      "Loss: 1.0011175870895386\n",
      "Loss: 1.0001109838485718\n",
      "Loss: 0.8320841193199158\n",
      "Loss: 0.9492046236991882\n",
      "Loss: 0.712446928024292\n",
      "Loss: 1.0574308633804321\n",
      "Loss: 0.8410575985908508\n",
      "Loss: 1.0444390773773193\n",
      "Loss: 0.7343554496765137\n",
      "Loss: 0.7305867075920105\n",
      "Loss: 1.0679513216018677\n",
      "Loss: 0.7347192168235779\n",
      "Loss: 0.5478178262710571\n",
      "Loss: 1.091119408607483\n",
      "Loss: 0.9160828590393066\n",
      "Loss: 0.9169924855232239\n",
      "Loss: 0.7399623990058899\n",
      "Loss: 0.7525563836097717\n",
      "Loss: 0.6358891129493713\n",
      "Loss: 1.0020978450775146\n",
      "Loss: 0.917198657989502\n",
      "Loss: 0.7521499395370483\n",
      "Loss: 0.9587412476539612\n",
      "Loss: 0.6722955107688904\n",
      "Loss: 0.8446435332298279\n",
      "Loss: 0.7093562483787537\n",
      "Loss: 0.7965041995048523\n",
      "Loss: 0.8855820298194885\n",
      "Loss: 0.719514012336731\n",
      "Loss: 0.713608980178833\n",
      "Loss: 0.8428829908370972\n",
      "Loss: 0.7009696960449219\n",
      "Loss: 0.929584801197052\n",
      "Loss: 0.6296724677085876\n",
      "Loss: 0.6379205584526062\n",
      "Loss: 0.8209040760993958\n",
      "Loss: 0.8541882038116455\n",
      "Loss: 0.5311025381088257\n",
      "Loss: 0.6241405606269836\n",
      "Loss: 0.6000263690948486\n",
      "Loss: 0.6884675621986389\n",
      "Loss: 0.6909184455871582\n",
      "Loss: 0.7459871172904968\n",
      "Loss: 0.5719636678695679\n",
      "Loss: 0.7253334522247314\n",
      "Loss: 0.631377637386322\n",
      "Loss: 0.7540953755378723\n",
      "Loss: 0.6645305156707764\n",
      "Loss: 0.5205023288726807\n",
      "Loss: 0.7078580260276794\n",
      "Loss: 0.9123882055282593\n",
      "Loss: 0.628879725933075\n",
      "Loss: 0.6767020225524902\n",
      "Loss: 0.5371099710464478\n",
      "Loss: 0.5371516942977905\n",
      "Loss: 0.933753252029419\n",
      "Loss: 0.7799347043037415\n",
      "Loss: 0.7185001373291016\n",
      "Loss: 0.6038442254066467\n",
      "Loss: 0.6091392636299133\n",
      "Loss: 0.7046358585357666\n",
      "Loss: 0.7408968210220337\n",
      "Loss: 0.686917781829834\n",
      "Loss: 0.8418653607368469\n",
      "Loss: 0.7444885969161987\n",
      "Loss: 0.8197812438011169\n",
      "Loss: 0.751049816608429\n",
      "Loss: 0.6022073030471802\n",
      "Loss: 0.7994081377983093\n",
      "Loss: 0.6091285943984985\n",
      "Loss: 0.6884503364562988\n",
      "Loss: 0.6361910700798035\n",
      "Loss: 0.5788509845733643\n",
      "Loss: 0.6124175786972046\n",
      "Loss: 0.5963863134384155\n",
      "Loss: 0.8503317832946777\n",
      "Loss: 0.7184902429580688\n",
      "Loss: 0.6325381994247437\n",
      "Loss: 0.6037746667861938\n",
      "Loss: 0.44677361845970154\n",
      "Loss: 0.8532183170318604\n",
      "Loss: 0.5536290407180786\n",
      "Loss: 0.8516114950180054\n",
      "Loss: 0.9937341213226318\n",
      "Loss: 0.9202098846435547\n",
      "Loss: 0.6865994930267334\n",
      "Loss: 0.623125433921814\n",
      "Loss: 0.5287214517593384\n",
      "Loss: 0.5954837203025818\n",
      "Loss: 0.5667592883110046\n",
      "Loss: 0.769575834274292\n",
      "Loss: 0.5899553894996643\n",
      "Loss: 0.6013861298561096\n",
      "Loss: 0.5111858248710632\n",
      "Loss: 0.8428676724433899\n",
      "Loss: 0.8075373768806458\n",
      "Loss: 0.5656329989433289\n",
      "Loss: 0.5304670929908752\n",
      "Loss: 0.5975183248519897\n",
      "Loss: 0.48095834255218506\n",
      "Loss: 0.946543276309967\n",
      "Loss: 0.5793853402137756\n",
      "Loss: 0.43332797288894653\n",
      "Loss: 0.4428829550743103\n",
      "Loss: 0.5691443085670471\n",
      "Loss: 0.5263822674751282\n",
      "Loss: 0.5507181286811829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-f7fe3df101dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w_1 = init_weights((32, 1, 5, 5))\n",
    "w_2 = init_weights((64, 32, 5, 5))\n",
    "w_3 = init_weights((128, 64, 2, 2))\n",
    "w_conv1 = [w_1, w_2, w_3]\n",
    "\n",
    "w_h2 = init_weights((128, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "#prelu constant\n",
    "a = init_weights((1,128))\n",
    "a_2 = init_weights((1,625))\n",
    "optimizer = RMSprop([*w_conv1, w_h2, w_o, a_2])\n",
    "\n",
    "# put this into a training loop over 100 epochs\n",
    "for (_, (X, y)) in enumerate(dataloader, 0):\n",
    "    optimizer.zero_grad()\n",
    "    noise = torch.tensor([])\n",
    "    for x in X:\n",
    "        noise = torch.cat((noise,model( x.reshape(784), w_conv1, w_h2, w_o, a_2, 0.8, 0.7))) #TODO: use batches\n",
    "    noise = noise.reshape((mb_size,10))\n",
    "    cost = torch.nn.functional.cross_entropy(noise, y)\n",
    "    cost.backward()\n",
    "    print(\"Loss: {}\".format(cost))\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
