{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## 1 Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "digits = load_digits ()\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]\n",
    "\n",
    "#data filtering \n",
    "num_1, num_2 = 3, 8\n",
    "mask = np.logical_or(target == num_1, target == num_2)\n",
    "data = data[mask]\n",
    "target = target[mask]\n",
    "\n",
    "#add column of 1's\n",
    "data = np.hstack((data,np.ones((len(data),1))))\n",
    "\n",
    "#relabel targets\n",
    "target[target == num_1] = 1\n",
    "target[target == num_2] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Classification with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperpar(lambdaSpace,k_splits=20):\n",
    "    #grid search\n",
    "    means=[]\n",
    "    for i in range(len(lambdaSpace)):\n",
    "        kf = KFold(n_splits=k_splits)\n",
    "        kf.get_n_splits(data)\n",
    "        scores=[]\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            logReg=LogisticRegression(C=lambdaSpace[i],solver='lbfgs')\n",
    "            logReg.fit(data[train_index],target[train_index])\n",
    "            scores.append(logReg.score(data[test_index],target[test_index]))\n",
    "        means.append(np.mean(scores))\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9859477124183007,\n",
       " 0.9915032679738562,\n",
       " 0.9942810457516339,\n",
       " 0.9942810457516339,\n",
       " 0.9942810457516339,\n",
       " 0.9942810457516339,\n",
       " 0.9942810457516339,\n",
       " 0.9972222222222221]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam=np.logspace(-2,5,8)\n",
    "hyperpar(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the means are all similar or even equal we choose $\\lambda=100$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def gradient(beta,X,y,lam=100):\n",
    "    if len(X.shape)>1:\n",
    "        return beta-lam/len(X)*np.sum(np.multiply(sigmoid(-np.multiply(y,X@beta)),np.multiply(y,X.T)),axis=1)\n",
    "    else:\n",
    "        return beta-lam/len(X)*sigmoid(-y*X@beta)*y*X.T\n",
    "    \n",
    "def predict(beta,X):\n",
    "    return X@beta\n",
    "\n",
    "def zero_one_loss(y_pred,y_truth):\n",
    "    return np.count_nonzero(y_pred!=y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4, 2, 0],\n",
       "        [4, 2, 2],\n",
       "        [4, 3, 3],\n",
       "        [4, 2, 4]]), array([ 1,  1, -1,  1]), array([4, 1, 4]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummy data\n",
    "#N=4,d=3\n",
    "y=np.array([1,1,-1,1])\n",
    "X=np.random.randint(0,5,size=(4,3))\n",
    "b=np.random.randint(1,5,3)\n",
    "X,y,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADAM(m,X,y,tau=1e-4,epsilon=1e-8,mu1=.9,mu2=.999,beta=0,lam=100):\n",
    "    #initialize with 0 see original paper\n",
    "    #(https://arxiv.org/pdf/1412.6980.pdf)\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    g=q=np.zeros(d)\n",
    "    for t in range(m):\n",
    "        #without replacement\n",
    "        index=np.random.randint(len(X))\n",
    "        l=gradient(beta,X[index],y[index])\n",
    "        g=(1-mu1)*l+mu1*g\n",
    "        q=(1-mu2)*np.square(l)+mu2*q\n",
    "        g_til=np.divide(g,1-mu1)\n",
    "        q_til=np.divide(q,1-mu2)\n",
    "        beta=beta-tau*np.divide(g_til,np.sqrt(q)+epsilon)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_average_gradient(m,X,y,tau_0,gamma,beta=0,lam=100):\n",
    "    #initialization\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    g_stored=-np.multiply(np.multiply(sigmoid(-np.multiply(y,X@beta)),y),X.T)\n",
    "    g=np.sum(g_stored,axis=1)/N\n",
    "    for t in range(m):\n",
    "        i=np.random.randint(N)\n",
    "        g_i=-y[i]*np.multiply(X[i].T,sigmoid(-y[i]*X[i]@beta))\n",
    "        g=g+(g_i-g_stored.T[i])/N\n",
    "        g_stored.T[i]=g_i\n",
    "        tau_t=tau_0/(1+gamma*t)\n",
    "        beta=beta*(1-tau_t/lam)-tau_t*g\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_coordinate_ascent(m,X,y,beta=0,lam=100,epsilon=1e-8):\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    alpha=np.random.uniform(size=N)\n",
    "    for t in range(m):\n",
    "        i=np.random.randint(N)\n",
    "        f_p=y[i]*X[i]@beta+np.log(alpha[i]/(1-alpha[i]))\n",
    "        f_pp=lam/N*X[i]@X[i].T+1/(alpha[i]*(1-alpha[i]))\n",
    "        next_alpha_i=np.clip(alpha[i]-f_p/f_pp,a_max=1-epsilon,a_min=epsilon)\n",
    "        beta=beta+lam/N*y[i]*X[i].T*(next_alpha_i-alpha[i])\n",
    "        alpha[i]=next_alpha_i\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(m,X,y,beta=0,lam=100):\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    z,y_weighted,W=None,None,None\n",
    "    for t in range(m):\n",
    "        z=X@beta\n",
    "        y_weighted=np.divide(y,sigmoid(y*z))\n",
    "        W=np.diag(lam/N*np.multiply(sigmoid(z),sigmoid(-z)))\n",
    "        beta=LA.inv(np.identity(d)+X.T@W@X)@X.T@W@(z+y_weighted)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dual does not work yet\n",
    "zero_one_loss(target,np.sign(data@dual_coordinate_ascent(450,data[:100],target[:100],np.zeros(65))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,X_test,y,y_test = train_test_split(data,target,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "Not all algorithms need all three hyper parameters\n",
    "* gradient descent needs $\\tau$ and $\\gamma$\n",
    "* stochastic gradient needs $\\tau$ and $\\gamma$\n",
    "* SG minibatch needs $\\tau$ and $\\gamma$\n",
    "* SG momentum needs $\\tau$, $\\gamma$ and $\\mu$\n",
    "* ADAM needs $\\tau$ (and $\\mu_1$ and $\\mu_2$ but they stay fixed)\n",
    "* stochastic average gradien needs $\\tau$ and $\\gamma$\n",
    "* dual coordinate as needs nothing\n",
    "* Newton nedds nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_space=np.logspace(-3,-1,3)\n",
    "mu_space=[.1,.2,.5]\n",
    "gamma_space=np.logspace(-4,-2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gerade erst gelesen man muss es sequentially machen. hab es aber exhaustive gemacht....\n",
    "def hyperSeach(func,spaces,X,y,m):\n",
    "    '''\n",
    "    func: Optimization method as function\n",
    "    spaces: a list of all hyper parameter spaces to be checked\n",
    "            needs to be in order: tau, gamma, mu (leave out what is not needed)\n",
    "    m: number of iterations\n",
    "    X: data\n",
    "    y: targets\n",
    "    \n",
    "    returns tuple of the best found hyper parameter in spaces\n",
    "    '''\n",
    "    N=len(spaces)\n",
    "    kf = KFold(n_splits=10)\n",
    "    hyper_par=[None]*N\n",
    "    best_error=np.inf\n",
    "    #perform exhaustive grid search\n",
    "    for hyper in itertools.product(*spaces):\n",
    "        error=0\n",
    "        for train_index ,validation_index in kf.split(X):\n",
    "            X_train ,X_validation = X[train_index],X[validation_index]\n",
    "            y_train ,y_validation = y[train_index],y[validation_index]\n",
    "            #optimize\n",
    "            beta=func(m,X_train,y_train,*list(hyper))\n",
    "            error+=zero_one_loss(y,np.sign(X@beta))\n",
    "        if error<best_error:\n",
    "            hyper_par=list(hyper)\n",
    "    return tuple(hyper_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters with lowest error rate: tau=0.10\n"
     ]
    }
   ],
   "source": [
    "#ADAM\n",
    "t=hyperSeach(ADAM,[tau_space],data,target,10)\n",
    "print('parameters with lowest error rate: tau=%.2f'%t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
