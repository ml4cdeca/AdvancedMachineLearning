{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## 1 Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "digits = load_digits ()\n",
    "data = digits[\"data\"]\n",
    "images = digits[\"images\"]\n",
    "target = digits[\"target\"]\n",
    "target_names = digits[\"target_names\"]\n",
    "\n",
    "#data filtering \n",
    "num_1, num_2 = 3, 8\n",
    "mask = np.logical_or(target == num_1, target == num_2)\n",
    "data = data[mask]\n",
    "target = target[mask]\n",
    "\n",
    "#add column of 1's\n",
    "data = np.hstack((data,np.ones((len(data),1))))\n",
    "\n",
    "#relabel targets\n",
    "target[target == num_1] = 1\n",
    "target[target == num_2] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Classification with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperpar(lambdaSpace,k_splits=20):\n",
    "    #grid search\n",
    "    means=[]\n",
    "    for i in range(len(lambdaSpace)):\n",
    "        kf = KFold(n_splits=k_splits)\n",
    "        kf.get_n_splits(data)\n",
    "        scores=[]\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            logReg=LogisticRegression(C=lambdaSpace[i],solver='lbfgs')\n",
    "            logReg.fit(data[train_index],target[train_index])\n",
    "            scores.append(logReg.score(data[test_index],target[test_index]))\n",
    "        means.append(np.mean(scores))\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9859477124183007,\n",
       " 0.9915032679738562,\n",
       " 0.9942810457516339,\n",
       " 0.9942810457516339,\n",
       " 0.9942810457516339,\n",
       " 0.9942810457516339,\n",
       " 0.9942810457516339,\n",
       " 0.9972222222222221]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam=np.logspace(-2,5,8)\n",
    "hyperpar(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the means are all similar or even equal we choose $\\lambda=100$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Optimization Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def gradient(beta,X,y,lam=100):\n",
    "    if len(X.shape)>1:\n",
    "        return beta-lam/len(X)*np.sum(np.multiply(sigmoid(-np.multiply(y,X@beta)),np.multiply(y,X.T)),axis=1)\n",
    "    else:\n",
    "        return beta-lam*sigmoid(-y*X@beta)*y*X.T\n",
    "    \n",
    "def predict(beta,X):\n",
    "    return np.sign(X@beta)\n",
    "\n",
    "def zero_one_loss(y_pred,y_truth):\n",
    "    return np.count_nonzero(y_pred!=y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4, 2, 2],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 2],\n",
       "        [3, 2, 4]]), array([ 1,  1, -1,  1]), array([3, 2, 1]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummy data\n",
    "#N=4,d=3\n",
    "y=np.array([1,1,-1,1])\n",
    "X=np.random.randint(0,5,size=(4,3))\n",
    "b=np.random.randint(1,5,3)\n",
    "X,y,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent(m, X, y, tau=.1, gamma=.01,beta=0, lam = 100):\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    for iteration in range(m):\n",
    "        beta = beta - tau/(1 + gamma*iteration) * gradient(beta,X,y)\n",
    "    return beta\n",
    "\n",
    "def SG(m, X, y, tau=.1, gamma=.01,beta=0, lam = 100):\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    for iteration in range(m):\n",
    "        instance = np.random.randint(N)\n",
    "        beta = beta - tau/(1 + gamma*iteration) * gradient(beta,X[instance],y[instance])\n",
    "    return beta\n",
    "\n",
    "def SG_minibatch(m, X, y, tau=.1, gamma=.01,batchsize=16,beta=0, lam = 100):\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    indices = np.arange(len(X))\n",
    "    for iteration in range(m):\n",
    "        instances = np.random.permutation(len(X))[:batchsize]\n",
    "        beta = beta - tau/(1 + gamma*iteration) * gradient(beta,X[instances],y[instances])\n",
    "    return beta\n",
    "\n",
    "def SG_momentum(m,X,y,tau=.1,gamma=.01,mu=.5,beta=0,lam=100):\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    g=np.zeros(d)\n",
    "    for t in range(m):\n",
    "        #choose random instance\n",
    "        i = np.random.randint(N)\n",
    "        g=mu*g+(1-mu)*gradient(beta,X[i],y[i])\n",
    "        beta=beta-tau/(1+gamma*t)*g\n",
    "    return beta\n",
    "\n",
    "def ADAM(m,X,y,tau=1e-4,epsilon=1e-8,mu1=.9,mu2=.999,beta=0,lam=100):\n",
    "    #initialize with 0 see original paper\n",
    "    #(https://arxiv.org/pdf/1412.6980.pdf)\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    g=q=np.zeros(d)\n",
    "    for t in range(m):\n",
    "        #without replacement\n",
    "        index=np.random.randint(len(X))\n",
    "        l=gradient(beta,X[index],y[index])\n",
    "        g=(1-mu1)*l+mu1*g\n",
    "        q=(1-mu2)*np.square(l)+mu2*q\n",
    "        g_til=np.divide(g,1-mu1)\n",
    "        q_til=np.divide(q,1-mu2)\n",
    "        beta=beta-tau*np.divide(g_til,np.sqrt(q)+epsilon)\n",
    "    return beta\n",
    "\n",
    "def stochastic_average_gradient(m,X,y,tau_0=.1,gamma=.01,beta=0,lam=100):\n",
    "    #initialization\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    g_stored=-np.multiply(np.multiply(sigmoid(-np.multiply(y,X@beta)),y),X.T)\n",
    "    g=np.sum(g_stored,axis=1)/N\n",
    "    for t in range(m):\n",
    "        i=np.random.randint(N)\n",
    "        g_i=-y[i]*np.multiply(X[i].T,sigmoid(-y[i]*X[i]@beta))\n",
    "        g=g+(g_i-g_stored.T[i])/N\n",
    "        g_stored.T[i]=g_i\n",
    "        tau_t=tau_0/(1+gamma*t)\n",
    "        beta=beta*(1-tau_t/lam)-tau_t*g\n",
    "    return beta\n",
    "\n",
    "def dual_coordinate_ascent(m,X,y,beta=0,lam=100,epsilon=1e-8):\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    alpha=np.random.uniform(size=N)\n",
    "    beta = lam/N * np.sum(np.multiply(np.multiply(alpha,y),X.T),axis = 1)\n",
    "    for t in range(m):\n",
    "        i=np.random.randint(N)\n",
    "        f_p=y[i]*X[i]@beta+np.log(alpha[i]/(1-alpha[i]))\n",
    "        f_pp=lam/N*X[i]@X[i].T+1/(alpha[i]*(1-alpha[i]))\n",
    "        next_alpha_i=np.clip(alpha[i]-f_p/f_pp,a_max=1-epsilon,a_min=epsilon)\n",
    "        beta=beta+lam/N*y[i]*X[i].T*(next_alpha_i-alpha[i])\n",
    "        alpha[i]=next_alpha_i\n",
    "    return beta\n",
    "\n",
    "def newton(m,X,y,beta=0,lam=100):\n",
    "    N,d=X.shape\n",
    "    if type(beta)!=np.ndarray:\n",
    "        beta=np.zeros(d) if beta==0 else np.array(beta)\n",
    "    z,y_weighted,W=None,None,None\n",
    "    for t in range(m):\n",
    "        z=X@beta\n",
    "        y_weighted=np.divide(y,sigmoid(y*z))\n",
    "        W=np.diag(lam/N*np.multiply(sigmoid(z),sigmoid(-z)))\n",
    "        beta=LA.inv(np.identity(d)+X.T@W@X)@X.T@W@(z+y_weighted)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,X_test,y,y_test = train_test_split(data,target,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate\n",
    "Not all algorithms need all three hyper parameters\n",
    "* gradient descent needs $\\tau$ and $\\gamma$\n",
    "* stochastic gradient needs $\\tau$ and $\\gamma$\n",
    "* SG minibatch needs $\\tau$ and $\\gamma$\n",
    "* SG momentum needs $\\tau$, $\\gamma$ and $\\mu$\n",
    "* ADAM needs $\\tau$ (and $\\mu_1$ and $\\mu_2$ but they stay fixed)\n",
    "* stochastic average gradient needs $\\tau$ and $\\gamma$\n",
    "* dual coordinate as needs nothing\n",
    "* Newton nedds nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_space=np.logspace(-3,-1,3)\n",
    "mu_space=[.1,.2,.5]\n",
    "gamma_space=np.logspace(-4,-2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperSeach(func,spaces,m,X,y):\n",
    "    '''\n",
    "    func: Optimization method as function\n",
    "    spaces: a list of all hyper parameter spaces to be checked\n",
    "            needs to be in order: tau, gamma, mu (leave out what is not needed)\n",
    "    m: number of iterations\n",
    "    X: data\n",
    "    y: targets\n",
    "    \n",
    "    returns tuple of the best found hyper parameter in spaces\n",
    "    '''\n",
    "    N=len(spaces)\n",
    "    kf = KFold(n_splits=10)\n",
    "    hyper_par=[None]*N\n",
    "    best_error=np.inf\n",
    "    #perform exhaustive grid search\n",
    "    for hyper in zip(*spaces):\n",
    "        error=0\n",
    "        for train_index ,validation_index in kf.split(X):\n",
    "            X_train ,X_validation = X[train_index],X[validation_index]\n",
    "            y_train ,y_validation = y[train_index],y[validation_index]\n",
    "            #optimize\n",
    "            beta=func(m,X_train,y_train,*list(hyper))\n",
    "            error+=zero_one_loss(y,np.sign(X@beta))\n",
    "        if error<best_error:\n",
    "            hyper_par=list(hyper)\n",
    "    return tuple(hyper_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters with lowest error rate: tau=0.10, gamma=0.01\n"
     ]
    }
   ],
   "source": [
    "#gradient descent\n",
    "t,g=hyperSeach(gradient_decent,[tau_space,gamma_space],150,data,target)\n",
    "print('parameters with lowest error rate: tau=%.2f, gamma=%.2f'%(t,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters with lowest error rate: tau=0.10, gamma=0.01\n"
     ]
    }
   ],
   "source": [
    "#SG\n",
    "t,g=hyperSeach(SG,[tau_space,gamma_space],150,data,target)\n",
    "print('parameters with lowest error rate: tau=%.2f, gamma=%.2f'%(t,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters with lowest error rate: tau=0.10, gamma=0.01\n"
     ]
    }
   ],
   "source": [
    "#SG minibatch\n",
    "t,g=hyperSeach(SG_minibatch,[tau_space,gamma_space],150,data,target)\n",
    "print('parameters with lowest error rate: tau=%.2f, gamma=%.2f'%(t,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters with lowest error rate: tau=0.10, gamma=0.01, mu=0.50\n"
     ]
    }
   ],
   "source": [
    "#SG momentum\n",
    "t,g,m=hyperSeach(SG_momentum,[tau_space,gamma_space,mu_space],150,data,target)\n",
    "print('parameters with lowest error rate: tau=%.2f, gamma=%.2f, mu=%.2f'%(t,g,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters with lowest error rate: tau=0.10\n"
     ]
    }
   ],
   "source": [
    "#ADAM\n",
    "t=hyperSeach(ADAM,[tau_space],10,data,target)\n",
    "print('parameters with lowest error rate: tau=%.2f'%t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters with lowest error rate: tau=0.10, gamma=0.01\n"
     ]
    }
   ],
   "source": [
    "#stochastic average gradient \n",
    "t,g=hyperSeach(ADAM,[tau_space,gamma_space],150,data,target)\n",
    "print('parameters with lowest error rate: tau=%.2f, gamma=%.2f'%(t,g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [gradient_decent,SG,SG_minibatch,SG_momentum,ADAM,stochastic_average_gradient,dual_coordinate_ascent,newton]\n",
    "train_err = [[]]*len(functions)\n",
    "test_err = [[]]*len(functions)\n",
    "iterations = np.arange(10)[1:]*100\n",
    "for i in range(len(functions)):\n",
    "    for m in iterations:\n",
    "        function = functions[i]\n",
    "        if i != 4 :  \n",
    "            beta = function(m,X,y)\n",
    "        else: #for ADAM\n",
    "            beta = function(m,X,y,.1)\n",
    "        train_err[i].append(np.sum(np.square(predict(beta,X)-y))/len(X))\n",
    "        test_err[i].append(np.sum(np.square(predict(beta,X_test)-y_test))/len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08032128514056225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3373493975903614, 2.0240963855421685, 0.1606425702811245, 0.1606425702811245, 0.2891566265060241, 0.8514056224899599, 0.08032128514056225, 0.08032128514056225, 0.321285140562249, 0.0963855421686747, 0.04819277108433735, 0.17670682730923695, 2.0240963855421685, 0.01606425702811245, 0.11244979919678715, 0.14457831325301204, 0.0, 0.0321285140562249, 0.2891566265060241, 0.14457831325301204, 0.321285140562249, 0.6746987951807228, 1.8313253012048192, 1.9598393574297188, 0.4497991967871486, 1.7349397590361446, 0.43373493975903615, 0.1285140562248996, 0.04819277108433735, 1.2690763052208835, 0.5140562248995983, 0.1606425702811245, 0.27309236947791166, 0.24096385542168675, 0.0642570281124498, 0.0963855421686747, 0.2891566265060241, 0.14457831325301204, 0.2570281124497992, 0.20883534136546184, 0.1606425702811245, 0.14457831325301204, 0.14457831325301204, 0.08032128514056225, 0.08032128514056225, 0.5301204819277109, 1.0281124497991967, 0.0963855421686747, 0.14457831325301204, 0.8353413654618473, 0.6746987951807228, 1.0441767068273093, 0.0963855421686747, 0.36947791164658633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
