{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True,num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts label idx (n labels total) into one-hot encoding\n",
    "def idx2onehot(idx, n):\n",
    "\n",
    "    assert torch.max(idx).item() < n\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "\n",
    "    onehot = torch.zeros(idx.size(0), n)\n",
    "    onehot.scatter_(1, idx, 1)\n",
    "\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics/Repetition  of VAE\n",
    "\n",
    "In the lecture we have shown that $$\\log p^*(x^{(i)}) \\ge  -  D_{KL}[p_E(z \\mid x^{(i)}) \\| p(z)] +\\mathbb{E}_{z\\sim p_E(z \\mid x^{(i)})} [\\log p_D (x^{(i)} \\mid z)] = -\\mathcal{L}(D, E, x^{(i)}),$$\n",
    "where $x^{(i)}\\in\\mathbb{R}^D$ is the $i$-th training instance (since the pixel values of MNIST images are in the range 0...1, we even have $x^{(i)}\\in[0,1]^D$ in this case). The LHS is the logarithm of the true data distribution, and the RHS is termed the \"evicence lower bound\" (ELBO).\n",
    "\n",
    "We call $p_E(z \\mid x)$ the encoder and $p_D( x \\mid z)$ the decoder. Both will be represented by neural networks. Our goal is to approximate $p^*(x)$ as well as possible by maximizing the ELBO or equivalently minimizing its negation. Specifically, we minimize $\\mathcal{L}(D, E, x^{(i)})$ with respect to the parameters of $D$ and $E$ via gradient descent over all training instances $i$. \n",
    "\n",
    "In order to estimate the negative ELBO, we approximate the expectation w.r.t. $z$ by its average over $L$ instances:\n",
    "\\begin{align} \\hat{\\mathcal{L}}(D, E, x^{(i)}) = D_{KL}\\left[p_E(z \\mid x^{(i)}) \\| p(z)\\right]+ \\frac{1}{L} \\sum_{l=1}^L \\left(-\\log p_D(x^{(i)} \\mid z^{(i,l)})\\right)\\end{align} \n",
    "where $z^{(i,l)} \\sim p_E(z \\mid x^{(i)}) $. By construction of a VAE, $p_E(z \\mid x^{(i)})$ is a Gaussian distribution whose mean $\\mu^{(i)}=\\mu_E(x^{(i)})$ and standard deviation $\\sigma^{(i)}=\\sigma_E(x^{(i)})$ are computed by the encoder network. For fixed $x^{(i)}$, we can draw samples $z^{(i,l)}$ from this code distribution by means of the reparametrization trick: \n",
    "$$z^{(i,l)}\\sim \\mathcal{N}\\big(\\mu^{(i)}, \\text{diag}(\\sigma^{(i)})^2\\big) \\Leftrightarrow  z^{(i,l)} = \\mu^{(i)} + \\epsilon_l \\cdot \\sigma^{(i)}$$ \n",
    "with $\\epsilon_l\\sim\\mathcal{N}(0, \\mathbb{I})$. Note that $\\mu^{(i)}$, $\\sigma^{(i)}$, and $\\epsilon_l$ are vectors of length equal to the dimension $J$ of the latent space, and $\\epsilon_l \\cdot \\sigma^{(i)}$ is element-wise multiplication. In practice, $L=1$ is usually sufficient.\n",
    "\n",
    "Furthermore, we assume that the latent prior is a standard normal distribution, i.e. $p(z) = \\mathcal{N}(0, \\mathbb{I})$. The KL-term can then be computed analytically:\n",
    "$$ D_{KL}\\left[p_E(z \\mid x^{(i)}) \\| p(z)\\right] = \\frac{1}{2} \\sum_{j=1}^J \\left((\\mu_j^{(i)})^2 + (\\sigma_j^{(i)})^2 - 2 \\log(\\sigma_j^{(i)}) - 1\\right) $$\n",
    "\n",
    "Likewise, we consider $p_D(x \\mid z)$ as a Gaussian distribution with mean $\\mu_D(z)$ and fixed covariance matrix $\\sigma_G^2\\cdot \\mathbb{I}$ (i.e. $\\sigma_G$ is the fixed noise standard deviation):\n",
    "$$ p_G(x \\mid z) = \\mathcal{N}\\big(\\mu_D(z), \\sigma_G^2\\cdot\\mathbb{I}\\big)$$\n",
    "To ensure that $\\mu_D(z) \\in [0,1]^D$ holds for reconstructed images (without noise), the decoder's output layer should use the sigmoid activation function. The second term in the negated ELBO (the negative log-likelihood) now reduces to the squared loss:\n",
    "$$-\\log p_D(x^{(i)} \\mid z^{(i,l)}) = \\frac{||x^{(i)} - \\mu_D(z^{(i,l)})||^2_2}{2 \\sigma_G^2}  + \\text{const.}$$\n",
    "The additive constant has no influence on the training optimimum and can be dropped. $\\sigma_G$ can be used as a hyperparameter to balance the two loss terms.\n",
    "\n",
    "For a batch of samples $X = (x^{(1)}, \\dots, x^{(M)})$, we finally get the negated ELBO as:\n",
    "\\begin{align} -ELBO = \\sum_{i=1}^M \\Big[&\\frac{1}{2} \\sum_{j=1}^J \\left((\\mu_j^{(i)})^2 + (\\sigma_j^{(i)})^2 - 2 \\log(\\sigma_j^{(i)}) - 1\\right) \\\\+& \\frac{1}{L} \\sum_{l=1}^L \\sum_{j=1}^D \\frac{(x^{(i)}_j - \\mu_D(z^{(i,l)})_j)^2}{2\\sigma_G^2}\\Big]\\end{align}\n",
    "Training is performed by gradient descent on this loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implementation of VAE and CVAE\n",
    "\n",
    "Complete the code below. The CVAE class consists of three parts\n",
    "* The Encoder class that implements $p_E (z \\mid x, y)$,\n",
    "* The Decoder class which implements $p_D (x \\mid z, y)$ and\n",
    "* The actual CVAE class that combines both encoder and decoder.\n",
    "\n",
    "The conditioning variable $y$ holds the labels, e.g. 0...9 for MNIST digits. It is added as an additional network input, i.e. the encoder computes $\\mu_E(x^{(i)}, y^{(i)})$ and $\\sigma_E(x^{(i)}, y^{(i)})$, and the decoder implements `recon_x`=$\\mu_D(z^{(i)}, y^{(i)})$, where $z^{(i)}$ is sampled using the reparametrization trick explained above. \n",
    "\n",
    "Implement all three classes. The arguments and outputs for each method are given in the docstrings. Make sure the CVAE class implements both the conditional VAE (CVAE) and the plain VAE, where the latter is obtained if the number of labels is just 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_dim, encoder_layer_sizes, decoder_layer_sizes, latent_dim, num_labels=10, conditional=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            inp_dim (int): dimension of input,\n",
    "            encoder_layer_sizes (list[int]): list of the sizes of the encoder layers,\n",
    "            decoder_layer_sizes (list[int]): list of the sizes of the decoder layers,\n",
    "            latent_dim (int): dimension of latent space/bottleneck,\n",
    "            num_labels (int): amount of labels (important for conditional VAE),,\n",
    "            conditional (bool): True if CVAE, else False\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super(CVAE, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.encoder = Encoder(encoder_layer_sizes, latent_dim, num_labels, conditional)\n",
    "        self.decoder = Decoder(decoder_layer_sizes, latent_dim, num_labels, conditional)\n",
    "        \n",
    "    def forward(self, x, c=None):\n",
    "        \"\"\"\n",
    "        Forward Process of whole VAE/CVAE. \n",
    "        Arguments:\n",
    "            x: tensor of dimension (batch_size, 1, 28, 28) or (batch_size, 28*28)\n",
    "            c: None or tensor of dimension (batch_size, 1)\n",
    "        Output: recon_x, means, log_var\n",
    "            recon_x: see explanation on second part of estimator above,\n",
    "            means: output of encoder,\n",
    "            log_var: output of encoder (logarithm of variance)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = x.view(-1,784)\n",
    "        \n",
    "        ################################\n",
    "        # TODO: YOUR CODE STARTS BELOW #\n",
    "        ################################\n",
    "        means,log_var=self.encoder(x,c)\n",
    "        self.means,self.log_var=means,log_var\n",
    "        z=self.sampling()\n",
    "        recon_x=self.decoder(z,c)\n",
    "        \n",
    "        ################################\n",
    "        #     YOUR CODE ENDS HERE      #\n",
    "        ################################\n",
    "\n",
    "        return recon_x, means, log_var\n",
    "        \n",
    "    def sampling(self, n=2, c=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            n (int): amount of samples (amount of elements in the latent space)\n",
    "            c (bool): condition\n",
    "        Output:\n",
    "            x_sampled: n randomly sampled elements of the output distribution\n",
    "        \"\"\"\n",
    "        ################################\n",
    "        # TODO: YOUR CODE STARTS BELOW #\n",
    "        ################################\n",
    "        eps=torch.randn(n).to(device)\n",
    "        x_sampled=self.means+torch.mul(eps,self.log_var)\n",
    "        ################################\n",
    "        #     YOUR CODE ENDS HERE      #\n",
    "        ################################\n",
    "        return x_sampled \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_dim, num_labels, conditional=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            layer_sizes (list[int]): list of sizes of layers of the encoder,\n",
    "            latent_dim (int): dimension of latent space, i.e. dimension out output of the encoder,\n",
    "            num_labels (int): amount of labels,\n",
    "            conditional (bool): True if CVAE and False if VAE\n",
    "        \"\"\"\n",
    "        \n",
    "        ################################\n",
    "        # TODO: YOUR CODE STARTS BELOW #\n",
    "        ################################\n",
    "        self.conditional=conditional\n",
    "        #if conditional is true, we will have an extra input\n",
    "        if conditional:\n",
    "            layer_sizes[0]+=1\n",
    "        modules=[]\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            modules.append(nn.Linear(layer_sizes[i],layer_sizes[i+1]))\n",
    "            modules.append(nn.ReLU())\n",
    "        self.mlp=nn.Sequential(*modules)\n",
    "        self.linear_mu=nn.Linear(layer_sizes[-1],latent_dim)\n",
    "        self.linear_lvar=nn.Linear(layer_sizes[-1],latent_dim)\n",
    "        ################################\n",
    "        #     YOUR CODE ENDS HERE      #\n",
    "        ################################\n",
    "    \n",
    "    def forward(self, x, c=None):  \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: tensor of dimension (batch_size, 1, 28, 28) or (batch_size, 28*28)\n",
    "            c: None or tensor of dimension (batch_size, 1)\n",
    "        Output:\n",
    "            means: tensor of dimension (batch_size, latent_dim),\n",
    "            log_var: tensor of dimension (batch_size, latent_dim)\n",
    "        \"\"\"\n",
    "        ################################\n",
    "        # TODO: YOUR CODE STARTS BELOW #\n",
    "        ################################\n",
    "        x=x.view(-1,784)\n",
    "        if self.conditional:\n",
    "            #concatenate data and condition\n",
    "            x=torch.cat([x,c.view(-1,1)],dim=1)\n",
    "        x=self.mlp(x)\n",
    "        means=self.linear_mu(x)\n",
    "        log_vars=self.linear_lvar(x)\n",
    "        ################################\n",
    "        #     YOUR CODE ENDS HERE      #\n",
    "        ################################        \n",
    "        return means, log_vars\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_dim, num_labels, conditional=False):     \n",
    "        super(Decoder, self).__init__()\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            layer_sizes (list[int]): list of sizes of layers of the decoder,\n",
    "            latent_dim (int): dimension of latent space, i.e. dimension out input of the decoder,\n",
    "            num_labels (int): amount of labels,\n",
    "            conditional (bool): True if CVAE and False if VAE\n",
    "        Output:\n",
    "            x: Parameters of gaussian distribution; only mu (see above)\n",
    "        \"\"\"\n",
    "\n",
    "        ################################\n",
    "        # TODO: YOUR CODE STARTS BELOW #\n",
    "        ################################\n",
    "        self.conditional=conditional\n",
    "        modules=[]\n",
    "        #if conditional:\n",
    "        #    layer_sizes[-1]+=1\n",
    "        #effective layersizes start at latent dimension\n",
    "        eff_layer_sizes=[latent_dim,*layer_sizes]\n",
    "        for i in range(len(eff_layer_sizes)-1):\n",
    "            modules.append(nn.Linear(eff_layer_sizes[i],eff_layer_sizes[i+1]))\n",
    "            if i!=len(eff_layer_sizes)-2:\n",
    "                modules.append(nn.ReLU())\n",
    "        modules.append(nn.Sigmoid())\n",
    "        self.mlp=nn.Sequential(*modules)\n",
    "        ################################\n",
    "        #     YOUR CODE ENDS HERE      #\n",
    "        ################################\n",
    "            \n",
    "    def forward(self, z, c=None):\n",
    "        \"\"\"\n",
    "        Argumetns:\n",
    "            z: tensor of dimension (batch_size, latent_dim)\n",
    "            c: None or tensor of dimension (batch_size, 1)\n",
    "        Outputs:\n",
    "            x: mu of gaussian distribution (reconstructed image from latent code z)\n",
    "        \"\"\"\n",
    "        ################################\n",
    "        # TODO: YOUR CODE STARTS BELOW #\n",
    "        ################################\n",
    "        if self.conditional:\n",
    "            #concatenate data and condition\n",
    "            z=torch.cat([z,c.view(-1,1)],dim=1)\n",
    "        x=self.mlp(z)\n",
    "        \n",
    "        \n",
    "        ################################\n",
    "        #     YOUR CODE ENDS HERE      #\n",
    "        ################################\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "We want to minimize the negated ELBO loss:\n",
    "$$\\hat{\\mathcal{L}}(D, E, x^{(i)}) = D_{KL}\\left[p_E(z \\mid x^{(i)}) \\| p(z)\\right]+ \\frac{1}{L} \\sum_{l=1}^L \\left(-\\log p_D(x^{(i)} \\mid z^{(i,l)})\\right)$$\n",
    "where $L=1$. The `loss_function` should implement this estimator, expanding the two terms as explained above.\n",
    "* Implement the loss function\n",
    "* Comment/explain how your code arises from the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Loss function for the VAE/CVAE\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        recon_x: reconstruced input\n",
    "        x: input,\n",
    "        mu, log_var: parameters of posterior (distribution of z given x)\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # TODO: YOUR CODE STARTS BELOW #\n",
    "    ################################\n",
    "    #ensure same dimensions\n",
    "    recon_x=recon_x.view(-1,784)\n",
    "    x=x.view(-1,784)\n",
    "    kl_divergence=.5*torch.sum(mu*mu+log_var.exp()-log_var-1)\n",
    "    reconstruction_loss=F.binary_cross_entropy(recon_x,x,reduction='sum')\n",
    "    return kl_divergence+reconstruction_loss\n",
    "    ################################\n",
    "    #     YOUR CODE ENDS HERE      #\n",
    "    ################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of VAE\n",
    "Before we can do funny things with our VAE, we train it with a bottleneck size of two. If everything has been implemented correctly, you should obtain an VAE after a few epochs that is able to generate recognizable MNIST samples. \n",
    "\n",
    "The number of layers as well as their dimensions do not have to be changed throughout this exercise. Better results might by achieved with more carefully selected hyperparameters.\n",
    "* Simply run the code to train the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [12672/60000 (21%)]\tLoss: 11999.741211\n",
      "Train Epoch: 0 [25472/60000 (42%)]\tLoss: 10845.431641\n",
      "Train Epoch: 0 [38272/60000 (64%)]\tLoss: 10147.484375\n",
      "Train Epoch: 0 [51072/60000 (85%)]\tLoss: 10325.995117\n",
      "====> Epoch: 0 Average loss: 178.0364\n",
      "Train Epoch: 1 [12672/60000 (21%)]\tLoss: 10225.998047\n",
      "Train Epoch: 1 [25472/60000 (42%)]\tLoss: 10166.659180\n",
      "Train Epoch: 1 [38272/60000 (64%)]\tLoss: 9479.464844\n",
      "Train Epoch: 1 [51072/60000 (85%)]\tLoss: 9322.103516\n",
      "====> Epoch: 1 Average loss: 152.2719\n",
      "Train Epoch: 2 [12672/60000 (21%)]\tLoss: 9523.051758\n",
      "Train Epoch: 2 [25472/60000 (42%)]\tLoss: 10528.110352\n",
      "Train Epoch: 2 [38272/60000 (64%)]\tLoss: 9376.283203\n",
      "Train Epoch: 2 [51072/60000 (85%)]\tLoss: 9393.300781\n",
      "====> Epoch: 2 Average loss: 147.2994\n",
      "Train Epoch: 3 [12672/60000 (21%)]\tLoss: 9075.066406\n",
      "Train Epoch: 3 [25472/60000 (42%)]\tLoss: 9160.859375\n",
      "Train Epoch: 3 [38272/60000 (64%)]\tLoss: 9262.867188\n",
      "Train Epoch: 3 [51072/60000 (85%)]\tLoss: 9276.912109\n",
      "====> Epoch: 3 Average loss: 144.1250\n",
      "Train Epoch: 4 [12672/60000 (21%)]\tLoss: 8380.974609\n",
      "Train Epoch: 4 [25472/60000 (42%)]\tLoss: 8604.839844\n",
      "Train Epoch: 4 [38272/60000 (64%)]\tLoss: 9447.430664\n",
      "Train Epoch: 4 [51072/60000 (85%)]\tLoss: 8968.536133\n",
      "====> Epoch: 4 Average loss: 142.3238\n",
      "Train Epoch: 5 [12672/60000 (21%)]\tLoss: 8947.134766\n",
      "Train Epoch: 5 [25472/60000 (42%)]\tLoss: 8752.839844\n",
      "Train Epoch: 5 [38272/60000 (64%)]\tLoss: 8817.057617\n",
      "Train Epoch: 5 [51072/60000 (85%)]\tLoss: 9089.721680\n",
      "====> Epoch: 5 Average loss: 140.5459\n",
      "Train Epoch: 6 [12672/60000 (21%)]\tLoss: 9666.537109\n",
      "Train Epoch: 6 [25472/60000 (42%)]\tLoss: 8887.855469\n",
      "Train Epoch: 6 [38272/60000 (64%)]\tLoss: 8291.936523\n",
      "Train Epoch: 6 [51072/60000 (85%)]\tLoss: 8915.330078\n",
      "====> Epoch: 6 Average loss: 139.5023\n",
      "Train Epoch: 7 [12672/60000 (21%)]\tLoss: 9159.011719\n",
      "Train Epoch: 7 [25472/60000 (42%)]\tLoss: 8958.025391\n",
      "Train Epoch: 7 [38272/60000 (64%)]\tLoss: 8923.846680\n",
      "Train Epoch: 7 [51072/60000 (85%)]\tLoss: 8938.531250\n",
      "====> Epoch: 7 Average loss: 138.2809\n",
      "Train Epoch: 8 [12672/60000 (21%)]\tLoss: 8757.925781\n",
      "Train Epoch: 8 [25472/60000 (42%)]\tLoss: 8374.052734\n",
      "Train Epoch: 8 [38272/60000 (64%)]\tLoss: 8741.211914\n",
      "Train Epoch: 8 [51072/60000 (85%)]\tLoss: 8708.152344\n",
      "====> Epoch: 8 Average loss: 137.4072\n",
      "Train Epoch: 9 [12672/60000 (21%)]\tLoss: 8698.672852\n",
      "Train Epoch: 9 [25472/60000 (42%)]\tLoss: 8799.014648\n",
      "Train Epoch: 9 [38272/60000 (64%)]\tLoss: 8807.334961\n",
      "Train Epoch: 9 [51072/60000 (85%)]\tLoss: 8897.022461\n",
      "====> Epoch: 9 Average loss: 136.7028\n",
      "Train Epoch: 10 [12672/60000 (21%)]\tLoss: 8848.595703\n",
      "Train Epoch: 10 [25472/60000 (42%)]\tLoss: 9341.109375\n",
      "Train Epoch: 10 [38272/60000 (64%)]\tLoss: 8689.926758\n",
      "Train Epoch: 10 [51072/60000 (85%)]\tLoss: 9020.390625\n",
      "====> Epoch: 10 Average loss: 136.4361\n",
      "Train Epoch: 11 [12672/60000 (21%)]\tLoss: 8695.727539\n",
      "Train Epoch: 11 [25472/60000 (42%)]\tLoss: 9014.008789\n",
      "Train Epoch: 11 [38272/60000 (64%)]\tLoss: 8210.760742\n",
      "Train Epoch: 11 [51072/60000 (85%)]\tLoss: 8883.587891\n",
      "====> Epoch: 11 Average loss: 135.7542\n",
      "Train Epoch: 12 [12672/60000 (21%)]\tLoss: 8498.289062\n",
      "Train Epoch: 12 [25472/60000 (42%)]\tLoss: 8476.422852\n",
      "Train Epoch: 12 [38272/60000 (64%)]\tLoss: 8471.360352\n",
      "Train Epoch: 12 [51072/60000 (85%)]\tLoss: 8107.406250\n",
      "====> Epoch: 12 Average loss: 134.8066\n",
      "Train Epoch: 13 [12672/60000 (21%)]\tLoss: 8626.786133\n",
      "Train Epoch: 13 [25472/60000 (42%)]\tLoss: 8649.418945\n",
      "Train Epoch: 13 [38272/60000 (64%)]\tLoss: 8567.062500\n",
      "Train Epoch: 13 [51072/60000 (85%)]\tLoss: 8435.135742\n",
      "====> Epoch: 13 Average loss: 134.6539\n",
      "Train Epoch: 14 [12672/60000 (21%)]\tLoss: 8739.124023\n",
      "Train Epoch: 14 [25472/60000 (42%)]\tLoss: 8386.470703\n",
      "Train Epoch: 14 [38272/60000 (64%)]\tLoss: 8425.125977\n",
      "Train Epoch: 14 [51072/60000 (85%)]\tLoss: 8638.792969\n",
      "====> Epoch: 14 Average loss: 134.0998\n"
     ]
    }
   ],
   "source": [
    "encoder_layer_sizes = [784, 512, 256]\n",
    "decoder_layer_sizes = [256, 512, 784]\n",
    "\n",
    "latent_dim = 2 \n",
    "vae = CVAE(inp_dim=784, encoder_layer_sizes=encoder_layer_sizes, decoder_layer_sizes=decoder_layer_sizes, latent_dim=latent_dim)\n",
    "vae = vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "# Training of the VAE\n",
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch,  mu, log_var = vae(x)\n",
    "        loss = loss_function(recon_batch,  x, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 99:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * train_loader.batch_size, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "epochs = 15 \n",
    "for epoch in range(epochs):\n",
    "    train(epoch)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check VAE\n",
    "Your model should be able to reproduce the input image, i.e. the output of the VAE should look similar to be input. \n",
    "\n",
    "Note that the quality of the reproduction will also depend on the dimension of the latent space. The quality of the reconstruction may not be fairly bad for a $2$ dimensional latent space. \n",
    "* Run the code to check if your model worked\n",
    "* How are the reconstructions different from the original?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(npimg, vmin=0, vmax=1, cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "_, x= next(enumerate(train_loader))\n",
    "samples = x[0].to(device)[40:]\n",
    "samples_rec,   _, _ = vae(samples)\n",
    "samples_rec = samples_rec.detach().cpu().view(-1,28,28)\n",
    "for i in range(0, 3):\n",
    "    plt.subplot(3,2,2*i+1)\n",
    "    plt.tight_layout()\n",
    "    imshow(samples[i,0])\n",
    "    plt.title(\"Ori. {}\".format(i))\n",
    "\n",
    "    plt.subplot(3, 2, 2*i+2)\n",
    "    plt.tight_layout()\n",
    "    imshow(samples_rec[i])\n",
    "    plt.title(\"Rec. {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model should be able to generate images that look similar to the samples of the MNIST dataset.\n",
    "* Run the code\n",
    "* Describe what you see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 28, 28]' is invalid for input of size 176",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-33563643b73d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 28, 28]' is invalid for input of size 176"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI8AAABjCAYAAACi5VNqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABOZJREFUeJzt3U+IFnUcx/H3J80CDwnpIUowSVo8dNCH8BRBBOpBD3XQixnGIiWdgw6Bl+gUSJFsJGUHkzxtUARR4EnzWSjToliDaElwrfASWMK3wwy2bo87s19nfGYfPy944Jln/jzfH/thnmdm9vmOIgKzjLuGXYAtXQ6PpTk8lubwWJrDY2kOj6VVhkfSEUmXJJ27yXxJOiRpWtJZSZuaL9O6qM6e531g6wLztwEbysc48M6tl2VLQWV4IuIk8McCi+wEjkbhFLBK0gNNFWjd1cR3ngeBX+dMz5Sv2Yhb3sA2NOC1gdc8JI1TfLSxcuXKzWNjYw28vd2qqampyxGxZrHrNRGeGWDtnOmHgN8GLRgRE8AEQK/Xi36/38Db262S9EtmvSY+tiaBPeVR1xbgSkRcbGC71nGVex5Jx4AngdWSZoDXgLsBIuIw8CmwHZgG/gKeb6tY65bK8ETE7or5AbzUWEW2ZPgMs6U5PJbm8Fiaw2NpDo+lOTyW5vBYmsNjaQ6PpTk8lubwWJrDY2kOj6U5PJbm8Fiaw2NpDo+lOTyW5vBYmsNjaQ6PpTk8lubwWFqt8EjaKunHsgfPKwPm75U0K+mb8vFC86Va19T5xegy4G3gaYrfpZ+RNBkR389b9HhEHGihRuuoOnuex4HpiPg5Iv4GPqLoyWN3uDrhqdt/55myrdwJSWsHzLcRUyc8dfrvfAKsi4jHgC+ADwZuSBqX1JfUn52dXVyl1jl1wlPZfycifo+Iq+Xku8DmQRuKiImI6EVEb82aRfcSso6pE54zwAZJD0taAeyi6Mlz3bwehDuAH5or0bqqTouVa5IOAJ8Dy4AjEXFe0kGgHxGTwMuSdgDXKJpf7m2xZusIDeuWSW4r1x2SpiKit9j1fIbZ0hweS3N4LM3hsTSHx9IcHktzeCzN4bE0h8fSHB5Lc3gszeGxNIfH0hweS3N4LM3hsTSHx9IcHktzeCzN4bE0h8fSHB5Lc3gsran+PPdIOl7OPy1pXdOFWvdUhmdOf55twEZgt6SN8xbbB/wZEY8AbwJvNF2odU9T/Xl28l9njBPAU5IGddewEdJUf57ry0TENeAKcH8TBVp3VTY6oF5/njrLIGkcGC8nr0o6V+P9u2w1cHnYRTTg0cxKdcJT2Z9nzjIzkpYD91F0y7hBREwAEwCS+pkf13fJKIwBinFk1mukP085/Vz5/FngyxhW+w27bZrqz/Me8KGkaYo9zq42i7ZuGFp/Hknj5cfYkjUKY4D8OIYWHlv6fHnC0loPzyhc2hiF2ydIOiLp0s1Oj6hwqBzjWUmbKjcaEa09KL5gXwDWAyuAb4GN85Z5EThcPt9FcRuCVutqYQx7gbeGXWvFOJ4ANgHnbjJ/O/AZxTm7LcDpqm22vecZhUsbI3H7hIg4yYBzb3PsBI5G4RSwal6L5P9pOzyjcGnjTrl9Qt1xXtd2eBq7tDFEjd0+oeMW/XdoOzyLubTBQpc2hqix2yd0XJ2/1Q3aDs8oXNq4U26fMAnsKY+6tgBXIuLigmvchm/524GfKI5YXi1fOwjsKJ/fC3wMTANfA+uHfWSSGMPrwHmKI7GvgLFh1zxgDMeAi8A/FHuZfcB+YH85XxT/9HcB+A7oVW3TZ5gtzWeYLc3hsTSHx9IcHktzeCzN4bE0h8fSHB5L+xcqszlgyvQhwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    plt.subplot(3,3,i)\n",
    "    sample = vae.sampling(n=2).detach().view(-1,28,28).cpu()\n",
    "    plt.tight_layout()\n",
    "    imshow(sample[0])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVAE(\n",
      "  (encoder): Encoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (linear_mu): Linear(in_features=256, out_features=2, bias=True)\n",
      "    (linear_lvar): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=512, out_features=784, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vae.sampling().shape\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CVAE\n",
    "We optimize in the following the CVAE (simpy run the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [128 x 3], m2: [2 x 256] at C:/w/1/s/tmp_conda_3.7_055306/conda/conda-bld/pytorch_1556690124416/work/aten/src\\THC/generic/THCTensorMathBlas.cu:268",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-367c76a57baa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Training of CVAE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-367c76a57baa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-6989fedad13d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, c)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_var\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlog_var\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mrecon_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-6989fedad13d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, z, c)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m#concatenate data and condition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [128 x 3], m2: [2 x 256] at C:/w/1/s/tmp_conda_3.7_055306/conda/conda-bld/pytorch_1556690124416/work/aten/src\\THC/generic/THCTensorMathBlas.cu:268"
     ]
    }
   ],
   "source": [
    "encoder_layer_sizes = [784, 512, 256]\n",
    "decoder_layer_sizes = [256, 512, 784]\n",
    "latent_dim = 2\n",
    "cvae = CVAE(inp_dim=784, encoder_layer_sizes=encoder_layer_sizes, decoder_layer_sizes=decoder_layer_sizes, latent_dim=latent_dim, conditional=True )\n",
    "\n",
    "cvae = cvae.to(device)\n",
    "optimizer = optim.Adam(cvae.parameters())\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    cvae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        x, y = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = cvae(x, y)\n",
    "        loss = loss_function(recon_batch, x, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "    \n",
    "# Training of CVAE\n",
    "for epoch in range(1, 15):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVAE(\n",
      "  (encoder): Encoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=785, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (linear_mu): Linear(in_features=256, out_features=2, bias=True)\n",
      "    (linear_lvar): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=512, out_features=784, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(cvae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check CVAE\n",
    "Check whether your CVAE is able to reconstruct certain images when conditioned on the label (simply run the code).\n",
    "* Is there a difference to the standard VAE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,l = next(iter(train_loader))\n",
    "for i in range(0, 10):\n",
    "    x_one_label = x[l==i][:2]\n",
    "\n",
    "    samples = x_one_label[:1].to(device)\n",
    "    labels= i* torch.ones(1).type(torch.long)\n",
    "    plt.subplot(5,4,2*i+1)\n",
    "    plt.tight_layout()\n",
    "    imshow(samples[0,0].cpu())\n",
    "    plt.title(\"Ori. {}\".format(i))\n",
    "    \n",
    "    samples_rec, _, _ = cvae(samples, c = labels)\n",
    "    samples_rec = samples_rec.detach().cpu().view(-1,28,28)\n",
    "\n",
    "    plt.subplot(5, 4, 2*i+2)\n",
    "    plt.tight_layout()\n",
    "    imshow(samples_rec[0])\n",
    "    plt.title(\"Rec. {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether your CVAE is able to generate images from the MNIST dataset distribution by sampling from the latent space and decode these latent codes (simply run the code).\n",
    "* How do the generated digits compare to those of the VAE?\n",
    "* Can you imagine why differences could arise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    label = i* torch.ones(2).type(torch.long)\n",
    "    sample = cvae.sampling(n=2, c=label).detach().view(-1,28,28).cpu()\n",
    "    plt.tight_layout()\n",
    "    imshow(sample[0])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Cond. {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Visualisation of Latent Space of VAE\n",
    "\n",
    "\n",
    "### Visualisation of output of decoder\n",
    "Make sure you use the VAE trained with $2$ latent dimensions.\n",
    "\n",
    "* Illustrate the 2 dimensional latent space by showing decoder output for different values in the latent space (see example on exercise sheet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# TODO #\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of latent space\n",
    "In the following, you should visualize the latent space directly.\n",
    "* Make a scatter plot in latent space, where each plotted point represents the mean of the latent code of a single image from the MNIST dataset. Color the points according to the image label.\n",
    "* What kind of shape should ideally arise?\n",
    "* What do you see in reality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# TODO #\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaknesses of the VAE\n",
    "* Find from your plot coordinates in the latent space that migh cause problems to the VAE if you decode this points. Explain your reasoning.\n",
    "* Illustrate the decoding of one of these points.\n",
    "* Describe what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# TODO #\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualisation of Latent Space of CVAE\n",
    "\n",
    "Make sure you use the CVAE trained with $2$ latent dimensions.\n",
    "\n",
    "### Visualisation of Latent Space via Decoder\n",
    "\n",
    "Repeat task 2 for the CVAE: \n",
    "* Illustrate the 2 dimensional latent space by showing the output of the decoder for different values in the latent space (see example on exercise sheet).\n",
    "* Make two or three of these plots, each conditioned on a fixed label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# TODO #\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of Latent Space via Decoder\n",
    "* Repeat the scatter plot from Task 2. For each sample, use the correct label as the condition. Color the points according to the label.\n",
    "* What difference do you see, compared to the standard VAE?\n",
    "* How to you explain this?\n",
    "* What does this mean for 'bad samples', as observed in Task 2?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# TODO #\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Generative Classifier\n",
    "We define our classifier as maximum a posteriori estimator and expand according to Bayes rule:\n",
    "\n",
    "$$ \\hat y= \\arg \\max_y p(y \\mid x) = \\arg \\max_y \\frac{p(x \\mid y)p(y)}{p(x)} = \\arg \\max_y \\log p(x \\mid y).$$\n",
    "\n",
    "where the last identity makes use of the fact that $p(y)=1/10$ is constant for all MNIST labels. We can approximate $\\log p(x \\mid y)$ by running the CVAE with each label $y$ in turn (see first section for formula). Use this approximation to construct a classifier. Evaluate the accuracy of this classifier on the test dataset. Note that the network was never trained as classifier, but is still able to perform the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# TODO #\n",
    "########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
